<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">

<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/32*32.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/32*32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/16*16.ico">
  <link rel="mask-icon" href="/images/32*32.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.zhaoxuanlang.cn","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":true},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"manual"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="A machine-Learning Learning notes">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/index.html">
<meta property="og:site_name" content="松鼠小筑">
<meta property="og:description" content="A machine-Learning Learning notes">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.imgur.com/zRvpK1r.png">
<meta property="og:image" content="https://i.imgur.com/Orti8J9.png">
<meta property="og:image" content="https://i.imgur.com/k006QFs.png">
<meta property="og:image" content="https://i.imgur.com/chW6p6H.png">
<meta property="og:image" content="https://i.imgur.com/5AyiP2i.png">
<meta property="og:image" content="https://i.imgur.com/ehen9BU.png">
<meta property="og:image" content="https://i.imgur.com/BR91KWS.png">
<meta property="og:image" content="https://i.imgur.com/EqBTzcH.png">
<meta property="og:image" content="https://i.imgur.com/gGdUowO.png">
<meta property="article:published_time" content="2022-09-09T15:00:24.000Z">
<meta property="article:modified_time" content="2022-11-03T15:18:45.890Z">
<meta property="article:author" content="Xuanlang">
<meta property="article:tag" content="松鼠小筑">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/zRvpK1r.png">


<link rel="canonical" href="https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/","path":"2022/09/09/Machine-Learning/","title":"Machine Learning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Machine Learning | 松鼠小筑</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?2c6ad69c6dce83b3987864c3d69796db"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">松鼠小筑</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">学习笔记</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">27</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">11</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">92</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#supervised-learning"><span class="nav-number">1.</span> <span class="nav-text">Supervised learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#linear-regression"><span class="nav-number">1.1.</span> <span class="nav-text">Linear regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model"><span class="nav-number">1.1.1.</span> <span class="nav-text">model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cost-function"><span class="nav-number">1.1.2.</span> <span class="nav-text">Cost function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-descent"><span class="nav-number">1.1.3.</span> <span class="nav-text">Gradient descent</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#algorithm"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multiple-linear-regression"><span class="nav-number">1.2.</span> <span class="nav-text">Multiple Linear regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">Model:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vectorization"><span class="nav-number">1.2.2.</span> <span class="nav-text">Vectorization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multiple-gradient-descent"><span class="nav-number">1.2.3.</span> <span class="nav-text">Multiple Gradient descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#feature-scaling"><span class="nav-number">1.2.4.</span> <span class="nav-text">Feature scaling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic-regression"><span class="nav-number">1.3.</span> <span class="nav-text">Logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#decision-boundary"><span class="nav-number">1.3.1.</span> <span class="nav-text">Decision boundary</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#costlost-function"><span class="nav-number">1.3.2.</span> <span class="nav-text">Cost&#x2F;Lost function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#overfitting"><span class="nav-number">1.3.3.</span> <span class="nav-text">overfitting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-decent"><span class="nav-number">1.3.4.</span> <span class="nav-text">Gradient decent</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decision-tree"><span class="nav-number">1.4.</span> <span class="nav-text">Decision tree</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#use-entropy-to-measure-purity"><span class="nav-number">1.4.1.</span> <span class="nav-text">use Entropy to measure
purity</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#neural-networks"><span class="nav-number">2.</span> <span class="nav-text">Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#neural-network-model"><span class="nav-number">2.1.</span> <span class="nav-text">Neural Network Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mutiple-layers-neural-networks"><span class="nav-number">2.2.</span> <span class="nav-text">Mutiple layers neural
networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#forward-propagation"><span class="nav-number">2.3.</span> <span class="nav-text">Forward propagation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#specific-calculation"><span class="nav-number">2.3.1.</span> <span class="nav-text">specific calculation:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#training-a-neural-network"><span class="nav-number">2.4.</span> <span class="nav-text">Training a neural network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#create-model"><span class="nav-number">2.4.1.</span> <span class="nav-text">Create model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#train-model"><span class="nav-number">2.4.2.</span> <span class="nav-text">Train model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#activation-functions"><span class="nav-number">2.5.</span> <span class="nav-text">Activation functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-choose"><span class="nav-number">2.5.1.</span> <span class="nav-text">How to choose?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multiclass-classification"><span class="nav-number">2.6.</span> <span class="nav-text">Multiclass Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax-regression"><span class="nav-number">2.6.1.</span> <span class="nav-text">Softmax regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cost"><span class="nav-number">2.6.2.</span> <span class="nav-text">Cost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#programming"><span class="nav-number">2.6.3.</span> <span class="nav-text">Programming</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#optimization-and-diagnosing"><span class="nav-number">2.7.</span> <span class="nav-text">Optimization and Diagnosing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adam-algorithm"><span class="nav-number">2.7.1.</span> <span class="nav-text">Adam algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#layer-types"><span class="nav-number">2.8.</span> <span class="nav-text">Layer types</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dense-layer"><span class="nav-number">2.8.1.</span> <span class="nav-text">Dense Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convolutional-layer"><span class="nav-number">2.8.2.</span> <span class="nav-text">Convolutional Layer</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#unsupervised-learning"><span class="nav-number">3.</span> <span class="nav-text">Unsupervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#pcaprinciple-component-analysis"><span class="nav-number">3.1.</span> <span class="nav-text">PCA(principle component
analysis)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#theoretical-part"><span class="nav-number">3.1.1.</span> <span class="nav-text">theoretical part</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#practical-part"><span class="nav-number">3.1.2.</span> <span class="nav-text">practical part</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kmeans"><span class="nav-number">3.2.</span> <span class="nav-text">Kmeans</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensorflow"><span class="nav-number">4.</span> <span class="nav-text">TensorFlow</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#simple-example"><span class="nav-number">4.1.</span> <span class="nav-text">Simple example:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#train-a-neural-network"><span class="nav-number">4.2.</span> <span class="nav-text">Train a neural network:</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xuanlang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Xuanlang</p>
  <div class="site-description" itemprop="description">业精于勤荒于嬉，行成于思毁于随</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">92</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/1982606762" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;1982606762" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zxl17302206700@gmail.com" title="E-Mail → mailto:zxl17302206700@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/xuanlang/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;xuanlang&#x2F;" rel="noopener" target="_blank"><i class="fa fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Xuanlang">
      <meta itemprop="description" content="业精于勤荒于嬉，行成于思毁于随">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="松鼠小筑">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine Learning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-09 17:00:24" itemprop="dateCreated datePublished" datetime="2022-09-09T17:00:24+02:00">2022-09-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-11-03 16:18:45" itemprop="dateModified" datetime="2022-11-03T16:18:45+01:00">2022-11-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2022/09/09/Machine-Learning/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/09/09/Machine-Learning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>27 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>A machine-Learning Learning notes</p>
<span id="more"></span>
<h1 id="supervised-learning">Supervised learning</h1>
<h2 id="linear-regression">Linear regression</h2>
<h3 id="model">model</h3>
<p><span class="math display">\[
f_{\omega,b}(x) = wx+b
\]</span></p>
<p>change the w and b to optimize the algorithm.</p>
<h3 id="cost-function">Cost function</h3>
<p>Used to evaluate the model. The goal is to use cost function to
minimize it and to find the best parameters for the model.</p>
<p>used for linear regression is <span class="math display">\[
J(\omega,b) =
\frac{1}{2m}\sum_{i=1}^{m}(f_{\omega,b}(x^{(i)})-y^{(i)})^2
\]</span> It's related with w and b</p>
<p>Graph when J is only related to w:</p>
<p><img
src="https://cdn.mathpix.com/snip/images/4PNilhlBG6Ar9npUJvpzHe04Da9TCX2XZTiS9frwkpk.original.fullsize.png" /></p>
<h3 id="gradient-descent">Gradient descent</h3>
<p>In a 3D plot which choose w,b as the ground,J(w,b)as the hight.To
find the best w,b it needs to go down from a "hill" to a "valley". You
look around and go step by step from the hill untill can't go down any
further, that's a valley.</p>
<h4 id="algorithm">algorithm</h4>
<p><span class="math display">\[
w=w-\alpha \frac{\partial}{\partial w} J(w, b)
\]</span> <span class="math display">\[
b=b-\alpha \frac{\partial}{\partial b} J(w, b)
\]</span></p>
<p><span class="math inline">\(\alpha\)</span> means "Learning rate",
which controls how largh you take for each step downhill.</p>
<p>You need to update w and b simultaneously, the correct method are as
follows: <span class="math display">\[
\begin{aligned}
&amp;t m p_{-} w=w-\alpha \frac{\partial}{\partial w} J(w, b) \\
&amp;{tmp_{-}b=b-\alpha \frac{\partial}{\partial b} J\left(w,b\right)}
\\
&amp;w=t m p_{-} w\\
&amp;b=tmp_{-}b
\end{aligned}
\]</span> Repeat doing this untile w and b convergence.</p>
<p>Why use derivative?</p>
<p>The learning rate is always positive,so if the derivative is
negative, to make the J(w,b) smaller you need to increase w. If the
derivative is positive, you need to decreate w.</p>
<p>The cost function usually shape like a bowl, so it only have one
minimum point.</p>
<h2 id="multiple-linear-regression">Multiple Linear regression</h2>
<p><span class="math inline">\(\mathrm{x}_j\)</span> = <span
class="math inline">\(j^{\text {th }}\)</span> feature</p>
<p><span
class="math inline">\(\overrightarrow{\mathrm{x}}^{(i)}\)</span> =
features of <span class="math inline">\(i^{\text {th }}\)</span>
training example, this is a vector</p>
<p><span class="math inline">\(\mathrm{X}_j^{(i)}\)</span> = value of
feature j in <span class="math inline">\(i^{\text {th }}\)</span>
training example</p>
<h3 id="model-1">Model:</h3>
<p><span class="math display">\[
f_{w, b}(x)=w_1 x_1+w_2 x_2+\cdots+w_n x_n+b
\]</span></p>
<p><span
class="math inline">\(\vec{\omega}=\left[\begin{array}{lllll}w_1 &amp;
w_2 &amp; w_3 &amp; \ldots &amp; w_n\end{array}\right]\)</span></p>
<p>It can be rewritten as:</p>
<p><span class="math inline">\(f_{\overrightarrow{\mathrm{w}},
b}(\overrightarrow{\mathrm{x}})=\overrightarrow{\mathrm{w}} \cdot
\overrightarrow{\mathrm{x}}+b\)</span></p>
<h3 id="vectorization">Vectorization</h3>
<p>overall: use numpy as much as possible to make the program run
faster.</p>
<p>Without vectorization you need to use for loop to calculate f</p>
<p>With vectorization you can use np.dot(w,x) to get the result.</p>
<h3 id="multiple-gradient-descent">Multiple Gradient descent</h3>
<p>Repeat{</p>
<p><span class="math inline">\(\left.w_1=w_1-\alpha \frac{1}{m}
\sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}},
b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right)
x_1^{(i)}\right)\)</span></p>
<p>​ =&gt;<span class="math inline">\(\frac{\partial}{\partial w_1}
J(\overrightarrow{\mathrm{w}}, b)\)</span></p>
<p><span class="math inline">\(\vdots\)</span></p>
<p><span class="math inline">\(w_n=w_n-\alpha \frac{1}{m}
\sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}},
b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right)
x_n^{(i)}\)</span></p>
<p><span class="math inline">\(b=b-\alpha \frac{1}{m}
\sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}},
b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right)\)</span></p>
<p>simultaneously update <span class="math inline">\(w_j(\)</span> for
<span class="math inline">\(j=1, \cdots, n)\)</span> and <span
class="math inline">\(b\)</span></p>
<p>}</p>
<h3 id="feature-scaling">Feature scaling</h3>
<p>Choose proper w1,w2...can make gradient descent faster.</p>
<h2 id="logistic-regression">Logistic regression</h2>
<p>use sigmoid function to make classifition.</p>
<p>Sigmoid function:</p>
<p><span class="math inline">\(g(z)=\frac{1}{1+e^{-z}} \quad
0&lt;g(z)&lt;1\)</span></p>
<p>Logistic regression model: <span class="math display">\[
z=\overrightarrow{\mathrm{w}} \cdot \overrightarrow{\mathrm{x}}+b
\\
g(z)=\frac{1}{1+e^{-z}}
\]</span></p>
<h3 id="decision-boundary">Decision boundary</h3>
<p>choose a threshold to determin whether <span
class="math inline">\(\hat y\)</span> is 0 or 1.</p>
<p>Normally use when <span
class="math inline">\(z=\overrightarrow{\mathrm{w}} \cdot
\overrightarrow{\mathrm{x}}+b=0\)</span> when it's a linear
situation.</p>
<p>When it's a non-linear situation, use this one <span
class="math inline">\(z=x_1^2+x_2^2-1=0\)</span></p>
<h3 id="costlost-function">Cost/Lost function</h3>
<p>Previous: <span class="math inline">\(J(\overrightarrow{\mathrm{w}},
b)=\frac{1}{m} \sum_{i=1}^m
\frac{1}{2}\left(f_{\overrightarrow{\mathrm{w}},
b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right)^2\)</span>
squared error.</p>
<p>It's cost function is a non-convex, so need a new function.</p>
<p>new version: <span class="math display">\[
L\left(f_{\overrightarrow{\mathrm{w}},
b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right),
y^{(i)}\right)=\left\{\begin{aligned}
-\log \left(f_{\overrightarrow{\mathrm{w}},
b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)\right) &amp; \text { if
} y^{(i)}=1 \\
-\log \left(1-f_{\overrightarrow{\mathrm{w}},
b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)\right) &amp; \text { if
} y^{(i)}=0
\end{aligned}\right.
\]</span></p>
<figure>
<img data-src="https://i.imgur.com/zRvpK1r.png"
alt="image-2022101512523502 AM" />
<figcaption aria-hidden="true">image-2022101512523502 AM</figcaption>
</figure>
<p>simplified version: <span class="math display">\[
L\left(f_{\overrightarrow{\mathrm{w}},
b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right),
y^{(i)}\right)=-y^{(i)} \log \left(f_{\overrightarrow{\mathrm{w}},
b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)\right)-\left(1-y^{(i)}\right)
\log \left(1-f_{\overrightarrow{\mathrm{w}},
b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)\right)
\]</span> It's cost function: <span class="math display">\[
J(\mathbf{w}, b)=\frac{1}{m}
\sum_{i=0}^{m-1}\left[\operatorname{loss}\left(f_{\mathbf{w},
b}\left(\mathbf{x}^{(i)}\right), y^{(i)}\right)\right]
\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_logistic</span>(<span class="params">X, y, w, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes cost</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n)): Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,)) : target values</span></span><br><span class="line"><span class="string">      w (ndarray (n,)) : model parameters  </span></span><br><span class="line"><span class="string">      b (scalar)       : model parameter</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      cost (scalar): cost</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        z_i = np.dot(X[i],w) + b</span><br><span class="line">        f_wb_i = sigmoid(z_i)</span><br><span class="line">        cost +=  -y[i]*np.log(f_wb_i) - (<span class="number">1</span>-y[i])*np.log(<span class="number">1</span>-f_wb_i)</span><br><span class="line">             </span><br><span class="line">    cost = cost / m</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="overfitting">overfitting</h3>
<ul>
<li>Underfit:the model doesn't fit the training set well.(high bias in
prediction result)</li>
<li>Overfit : the model fits the training set extremely well.(high
variance in result, small change cause huge difference)</li>
</ul>
<p>How to deal with it?</p>
<ul>
<li>collect more training examples</li>
<li>Reduce features to use</li>
<li>reduce the size of parameters(regularization)</li>
</ul>
<h3 id="gradient-decent">Gradient decent</h3>
<p>repeat until convergence: { <span class="math display">\[
\begin{aligned}
&amp;b:=b-\alpha \frac{\partial J(\mathbf{w}, b)}{\partial b} \\
&amp;w_j:=w_j-\alpha \frac{\partial J(\mathbf{w}, b)}{\partial w_j}
\quad \text { for } \mathrm{j}:=0 . . \mathrm{n}-1
\end{aligned}
\]</span> } <span class="math display">\[
\begin{gathered}
\frac{\partial J(\mathbf{w}, b)}{\partial b}=\frac{1}{m}
\sum_{i=0}^{m-1}\left(f_{\mathbf{w},
b}\left(\mathbf{x}^{(i)}\right)-\mathbf{y}^{(i)}\right) \\
\frac{\partial J(\mathbf{w}, b)}{\partial w_j}=\frac{1}{m}
\sum_{i=0}^{m-1}\left(f_{\mathbf{w},
b}\left(\mathbf{x}^{(i)}\right)-\mathbf{y}^{(i)}\right) x_j^{(i)}
\end{gathered}
\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_gradient_logistic</span>(<span class="params">X, y, w, b</span>):</span> </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the gradient for linear regression </span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n): Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,)): target values</span></span><br><span class="line"><span class="string">      w (ndarray (n,)): model parameters  </span></span><br><span class="line"><span class="string">      b (scalar)      : model parameter</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. </span></span><br><span class="line"><span class="string">      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    dj_dw = np.zeros((n,))                           <span class="comment">#(n,)</span></span><br><span class="line">    dj_db = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb_i = sigmoid(np.dot(X[i],w) + b)          <span class="comment">#(n,)(n,)=scalar</span></span><br><span class="line">        err_i  = f_wb_i  - y[i]                       <span class="comment">#scalar</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      <span class="comment">#scalar</span></span><br><span class="line">        dj_db = dj_db + err_i</span><br><span class="line">    dj_dw = dj_dw/m                                   <span class="comment">#(n,)</span></span><br><span class="line">    dj_db = dj_db/m                                   <span class="comment">#scalar</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dj_db, dj_dw  </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_</span>):</span> </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Performs batch gradient descent to learn theta. Updates theta by taking </span></span><br><span class="line"><span class="string">    num_iters gradient steps with learning rate alpha</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X :    (array_like Shape (m, n)</span></span><br><span class="line"><span class="string">      y :    (array_like Shape (m,))</span></span><br><span class="line"><span class="string">      w_in : (array_like Shape (n,))  Initial values of parameters of the model</span></span><br><span class="line"><span class="string">      b_in : (scalar)                 Initial value of parameter of the model</span></span><br><span class="line"><span class="string">      cost_function:                  function to compute cost</span></span><br><span class="line"><span class="string">      alpha : (float)                 Learning rate</span></span><br><span class="line"><span class="string">      num_iters : (int)               number of iterations to run gradient descent</span></span><br><span class="line"><span class="string">      lambda_ (scalar, float)         regularization constant</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      w : (array_like Shape (n,)) Updated values of parameters of the model after</span></span><br><span class="line"><span class="string">          running gradient descent</span></span><br><span class="line"><span class="string">      b : (scalar)                Updated value of parameter of the model after</span></span><br><span class="line"><span class="string">          running gradient descent</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># number of training examples</span></span><br><span class="line">    m = <span class="built_in">len</span>(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># An array to store cost J and w&#x27;s at each iteration primarily for graphing later</span></span><br><span class="line">    J_history = []</span><br><span class="line">    w_history = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the gradient and update the parameters</span></span><br><span class="line">        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update Parameters using w, b, alpha and gradient</span></span><br><span class="line">        w_in = w_in - alpha * dj_dw               </span><br><span class="line">        b_in = b_in - alpha * dj_db              </span><br><span class="line">       </span><br><span class="line">        <span class="comment"># Save cost J at each iteration</span></span><br><span class="line">        <span class="keyword">if</span> i&lt;<span class="number">100000</span>:      <span class="comment"># prevent resource exhaustion </span></span><br><span class="line">            cost =  cost_function(X, y, w_in, b_in, lambda_)</span><br><span class="line">            J_history.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span></span><br><span class="line">        <span class="keyword">if</span> i% math.ceil(num_iters/<span class="number">10</span>) == <span class="number">0</span> <span class="keyword">or</span> i == (num_iters-<span class="number">1</span>):</span><br><span class="line">            w_history.append(w_in)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Iteration <span class="subst">&#123;i:<span class="number">4</span>&#125;</span>: Cost <span class="subst">&#123;<span class="built_in">float</span>(J_history[-<span class="number">1</span>]):<span class="number">8.2</span>f&#125;</span>   &quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> w_in, b_in, J_history, w_history <span class="comment">#return w and J,w history for graphing</span></span><br></pre></td></tr></table></figure>
<h2 id="decision-tree">Decision tree</h2>
<p>A predict which features are discrete values.</p>
<p>Questions about the algorithm:</p>
<ol type="1">
<li>How to choose parameters?
<ol type="1">
<li>Maximize purity</li>
</ol></li>
<li>When to stop splitting
<ol type="1">
<li>When a node is 100% one class</li>
<li>When splitting a node will result in the tree exceeding a maximum
depth</li>
<li>When improvements in purity score are below a threshold</li>
</ol></li>
</ol>
<h3 id="use-entropy-to-measure-purity">use Entropy to measure
purity</h3>
<figure>
<img data-src="https://i.imgur.com/Orti8J9.png"
alt="image-2022110314327111 PM" />
<figcaption aria-hidden="true">image-2022110314327111 PM</figcaption>
</figure>
<p><span class="math inline">\(\begin{aligned} H\left(p_1\right)
&amp;=-p_1 \log _2\left(p_1\right)-p_0 \log _2\left(p_0\right) \\
&amp;=-p_1 \log _2\left(p_1\right)-\left(1-p_1\right) \log
_2\left(1-p_1\right) \end{aligned}\)</span></p>
<h1 id="neural-networks">Neural Networks</h1>
<h2 id="neural-network-model">Neural Network Model</h2>
<p>activation function:<span
class="math inline">\(a=f(x)=\frac{1}{1+e^{-(w x+b)}}\)</span></p>
<p>If a vector X begin input into the first layer which consists three
neurons.</p>
<p>Each neuron will return <span
class="math inline">\(a_1=g(\overrightarrow{\mathrm{w}} \cdot
\overrightarrow{\mathrm{x}}+b)*weight\)</span> and composits a vector of
three numbers, which is the output of this layer.</p>
<p><span class="math inline">\(w_1^{\text {[1] }}\)</span> denotes the
first layer's first w.</p>
<h2 id="mutiple-layers-neural-networks">Mutiple layers neural
networks</h2>
<p>Notation:</p>
<p><span class="math inline">\(a_j^{[l]}=g\left(\vec{w}_j^{[l]} \cdot
\vec{a}^{[l-1]}+b_j^{[l]}\right)\)</span></p>
<figure>
<img data-src="https://i.imgur.com/k006QFs.png"
alt="image-20221018111702509 AM" />
<figcaption aria-hidden="true">image-20221018111702509 AM</figcaption>
</figure>
<h2 id="forward-propagation">Forward propagation</h2>
<figure>
<img data-src="https://i.imgur.com/chW6p6H.png"
alt="image-2022101815035177 PM" />
<figcaption aria-hidden="true">image-2022101815035177 PM</figcaption>
</figure>
<h3 id="specific-calculation">specific calculation:</h3>
<p><span class="math inline">\(x=n p \cdot
\operatorname{array}([200,17])\)</span></p>
<p><span
class="math inline">\(a_1^{[1]}=g\left(\overrightarrow{\mathrm{w}}_1^{[1]}
\cdot \overrightarrow{\mathrm{x}}+b_1^{[1]}\right)\)</span></p>
<p>w1_1 <span class="math inline">\(=\)</span> np. array <span
class="math inline">\(([1,2])\)</span> b1_1 <span
class="math inline">\(=\)</span> np. <span
class="math inline">\(\operatorname{array}([-1])\)</span> <span
class="math inline">\(z 11=n p \cdot \operatorname{dot}(w 11, x)+b
11\)</span> a1_1 = sigmoid <span class="math inline">\(\left(z
1_{-1}\right)\)</span></p>
<p>and then a1 = np. array( [a1_1,a1_2,a1_3]</p>
<h2 id="training-a-neural-network">Training a neural network</h2>
<ol type="1">
<li>specify how to compute output</li>
<li>specify loss and cost</li>
<li>Train to minimize lost</li>
</ol>
<h3 id="create-model">Create model</h3>
<p>use sequence and dense func</p>
<h3 id="train-model">Train model</h3>
<p>repeat { <span class="math display">\[
\begin{aligned}
&amp;w_j^{[l]}=w_j^{[l]}-\alpha \frac{\partial}{\partial w_j}
J(\overrightarrow{\mathrm{w}}, b) \\
&amp;b_j^{[l]}=b_j^{[l]}-\alpha \frac{\partial}{\partial b j}
J(\overrightarrow{\mathrm{w}}, b)
\end{aligned}
\]</span> <span class="math display">\[
\text { \} }
\]</span></p>
<p><code>model.fit(X,y,epochs=100)</code></p>
<h2 id="activation-functions">Activation functions</h2>
<p>Linear activation function <span
class="math inline">\(g(z)=z\)</span></p>
<p>Sigmoid <span
class="math inline">\(g(z)=\frac{1}{1+e^{-z}}\)</span></p>
<p>ReLU <span class="math inline">\(g(z)=\max (0, z)\)</span></p>
<figure>
<img data-src="https://i.imgur.com/5AyiP2i.png"
alt="image-2022101985528546 PM" />
<figcaption aria-hidden="true">image-2022101985528546 PM</figcaption>
</figure>
<h3 id="how-to-choose">How to choose?</h3>
<p>output layer:</p>
<p>Binary cassification: Signoid</p>
<p>Regression:Linear activation</p>
<p>Regression with all positive:ReLU</p>
<p>Hidden layer</p>
<p>in hidden layer mostly ReLU</p>
<h2 id="multiclass-classification">Multiclass Classification</h2>
<h3 id="softmax-regression">Softmax regression</h3>
<p><span class="math inline">\(z_1=\overrightarrow{\mathrm{w}}_1 \cdot
\overrightarrow{\mathrm{x}}+b_1\)</span></p>
<p><span
class="math inline">\(a_1=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}\)</span>
<span class="math display">\[
z_j=\overrightarrow{\mathrm{w}}_j \cdot \overrightarrow{\mathrm{x}}+b_j
\quad \mathrm{j}=1, \ldots, \mathrm{N}
\]</span></p>
<p><span class="math display">\[
a_j=\frac{e^{z_j}}{\sum_{k=1}^N e^{z_k}}=\mathrm{P}(\mathrm{y}=j \mid
\overrightarrow{\mathrm{x}})
\]</span></p>
<h3 id="cost">Cost</h3>
<p><span class="math display">\[
a_N=\frac{e^{z_N}}{e^{z_1}+e^{z_2}+\cdots+e^{z_N}}=P(y=N \mid
\overrightarrow{\mathrm{x}})
\]</span></p>
<p><span class="math display">\[
\operatorname{loss}\left(a_1, \ldots, a_N,
y\right)=\left\{\begin{array}{lc}
-\log a_1 &amp; \text { if } y=1 \\
-\log a_2 &amp; \text { if } y=2 \\
&amp; \vdots \\
-\log a_N &amp; \text { if } y=N
\end{array}\right.
\]</span></p>
<h3 id="programming">Programming</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Dense(units=<span class="number">10</span>,activation=<span class="string">&#x27;softmax&#x27;</span>)<span class="comment">#output layer</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=SparseCategoricalCrossentropy())</span><br></pre></td></tr></table></figure>
<p>Better way(recommended):</p>
<p>To make it more numerically accurate.</p>
<p>From <span class="math inline">\(\operatorname{loss}=-y \log
(a)-(1-y) \log (1-a)\)</span></p>
<p>to <span class="math inline">\(\operatorname{loss}=-y \log
\left(\frac{1}{1+e^{-z}}\right)-(1-y) \log
\left(1-\frac{1}{1+e^{-z}}\right)\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Dense(units=<span class="number">10</span>,activation=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=SparseCategoricalCrossEntropy(from_logits=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<h2 id="optimization-and-diagnosing">Optimization and Diagnosing</h2>
<h3 id="adam-algorithm">Adam algorithm</h3>
<p>Adaptive Moment estimation</p>
<p>automatically change alpha while doing learning.Has a lot of alpha
for different parameters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">compile</span>(tf.keras.optimizers.Adam(learning_rate=<span class="number">1e-3</span>))</span><br></pre></td></tr></table></figure>
<h2 id="layer-types">Layer types</h2>
<h3 id="dense-layer">Dense Layer</h3>
<p>output is a function of all the activation function result</p>
<h3 id="convolutional-layer">Convolutional Layer</h3>
<p>Each neuron only looks at part of the previous layer's output</p>
<h1 id="unsupervised-learning">Unsupervised Learning</h1>
<h2 id="pcaprinciple-component-analysis">PCA(principle component
analysis)</h2>
<h3 id="theoretical-part">theoretical part</h3>
<p>It's propose is to reduce dimension of input data. Suppose we have
sample X with n dimensions <span class="math inline">\(X=\left\{x_0,
x_1, \ldots, x_m\right\}\)</span> and we want to have a transformation
<span class="math inline">\(y=P x\)</span> in which P is a matrix. Then
we get some data Y with k dimensions <span
class="math inline">\(Y=\left\{y_0, y_1, \ldots, y_m\right\}\)</span>
.</p>
<ol type="1">
<li><p>preprocessing data</p>
<p>normolize input data by subtracting their mean of each
column.</p></li>
<li><p>Do the PCA as follows:</p></li>
</ol>
<p>we need to make sure these two laws of PCA:</p>
<ul>
<li>After reducing, each of the dimensions should be independent, which
means every axis are orthogonal after PCA.</li>
<li>maximize the variance of each dimensions, which means keeping the
original data as much as possible.</li>
</ul>
<p>First we can denote the covariance matrix after transportation as
<span class="math inline">\(B_{k \times k}=\frac{1}{m} Y Y^T\)</span> ,
to make each dimensions independent B should be a diagonal matrix, which
means it's all 0 except for it's diagonal.</p>
<p>Let's substitute this equation <span class="math inline">\(y=P
x\)</span> into the equation <span class="math inline">\(B_{k \times
k}=\frac{1}{m} Y Y^T\)</span> and get: <span class="math display">\[
B_{k \times k}=\frac{1}{m} Y Y^T=\frac{1}{m} P X(P X)^T=P \frac{1}{m} X
X^T P^T=P_{k \times n} C_{n \times n} P_{n \times k}^T
\]</span> in which <span class="math inline">\(C_{n \times
n}=\frac{1}{m} X X^T\)</span> is the covariance matrix of training
data.</p>
<p>Use eigenvalue decomposition on C and get <span
class="math inline">\(D_{n \times n}=Q_{n \times n} C_{n \times n} Q_{n
\times n}^T\)</span> and D is a diagonal matrix.</p>
<p>So we can know P is a matrix composed by k-th big eigenvectors in
line. Each item in B is the eigenvalues sorted in descending order.
Eigenvalues shows the degree of confidence of each eigenvectors.</p>
<h3 id="practical-part">practical part</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="literal">None</span>, copy=<span class="literal">True</span>, whiten=<span class="literal">False</span>)</span><br><span class="line">pca.fit(X)</span><br></pre></td></tr></table></figure>
<p>parameters:</p>
<blockquote>
<p>n_ components : the component number needs to be kept in the
result.</p>
<p>​ None: keep all components ​ int : number of components ​ String:
choose components automatically</p>
<p>copy: whether keep the original data</p>
<p>​ true : original data keep same ​ false: original data changes into
lower dimension</p>
<p>whiten: if whiten the data</p>
</blockquote>
<p>pca's parameters :</p>
<blockquote>
<p>components_ : return those includes max variance.</p>
<p>explained_variance_ratio : return their variance's percentage</p>
<p>n_ components_ :return the number of components</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA()</span><br><span class="line">pca.fit(normed)</span><br><span class="line">x = pca.components_</span><br><span class="line">pca_2_compo = pca.components_[<span class="number">0</span>:<span class="number">2</span>, :]</span><br><span class="line">mapped_input = np.dot(normed, pca_2_compo.T)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">explained_variance = pca.explained_variance_ratio_</span><br><span class="line">top_10 = np.<span class="built_in">sum</span>(explained_variance[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of components&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;explained variance&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Explained Variance&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>,<span class="number">65</span>), np.log(pca.explained_variance_ratio_), <span class="string">&#x27;r--&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of components&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;eigenspectrum&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Eigenspectrum&#x27;</span>)</span><br><span class="line"></span><br><span class="line">top5 = pca.components_[:<span class="number">5</span>]</span><br><span class="line">top5 = top5.reshape(<span class="number">5</span>,<span class="number">8</span>,<span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">5</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(top5[i], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img data-src="https://i.imgur.com/ehen9BU.png"
alt="image-2022101813109545 AM" />
<figcaption aria-hidden="true">image-2022101813109545 AM</figcaption>
</figure>
<figure>
<img data-src="https://i.imgur.com/BR91KWS.png"
alt="image-2022101813118956 AM" />
<figcaption aria-hidden="true">image-2022101813118956 AM</figcaption>
</figure>
<figure>
<img data-src="https://i.imgur.com/EqBTzcH.png"
alt="image-2022101813125171 AM" />
<figcaption aria-hidden="true">image-2022101813125171 AM</figcaption>
</figure>
<h2 id="kmeans">Kmeans</h2>
<p>used for clustering.</p>
<h1 id="tensorflow">TensorFlow</h1>
<h2 id="simple-example">Simple example:</h2>
<figure>
<img data-src="https://i.imgur.com/gGdUowO.png"
alt="image-2022101835321992 PM" />
<figcaption aria-hidden="true">image-2022101835321992 PM</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">200.0</span>,<span class="number">17.0</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">3</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">a1 = layer_1(x)</span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">a2 = layer_2(a1)</span><br><span class="line"><span class="keyword">if</span> a2 &gt;= <span class="number">0.5</span>:</span><br><span class="line">  yhat = <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  yhat = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h2 id="train-a-neural-network">Train a neural network:</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layer_1 = Dense(units=<span class="number">3</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">model = Sequential([layer_1,layer_2])</span><br><span class="line">model.<span class="built_in">compile</span>(...)</span><br><span class="line">model.fit(x,y)<span class="comment">#x is data, y is label</span></span><br><span class="line">model.predict(x_new)<span class="comment"># get the new data </span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Or in a simple way:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential([</span><br><span class="line">  Dense(units=<span class="number">3</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">  Dense(units=<span class="number">1</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>in compile we need to specify loss function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line"> loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    optimizer=tf.keras.optimizers.Adam(<span class="number">0.01</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Xuanlang
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/" title="Machine Learning">https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/08/16/conda%E7%9B%B8%E5%85%B3%E4%BD%BF%E7%94%A8/" rel="prev" title="conda相关使用">
                  <i class="fa fa-chevron-left"></i> conda相关使用
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/01/06/monitor-csgo-item-price/" rel="next" title="monitor-csgo-item-price">
                  monitor-csgo-item-price <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">津ICP备19008018号-1 </a>
  </div>

<div class="copyright">
  &copy; 2019 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-award"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xuanlang</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/lozad@1.16.0/dist/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","single_dollars":true,"cjk_width":0.9,"normal_width":0.6,"append_css":true,"every_page":true,"js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/quicklink@2.2.0/dist/quicklink.umd.js" integrity="sha256-4kQf9z5ntdQrzsBC3YSHnEz02Z9C1UeW/E9OgnvlzSY=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"squirrel","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">

<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/32*32.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/32*32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/16*16.ico">
  <link rel="mask-icon" href="/images/32*32.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.zhaoxuanlang.cn","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":true},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"manual"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"changyan","storage":true,"lazyload":false,"nav":null,"activeClass":"changyan"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="A machine-Learning Learning notes">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/index.html">
<meta property="og:site_name" content="松鼠小筑">
<meta property="og:description" content="A machine-Learning Learning notes">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.mathpix.com/snip/images/4PNilhlBG6Ar9npUJvpzHe04Da9TCX2XZTiS9frwkpk.original.fullsize.png">
<meta property="og:image" content="https://i.imgur.com/zRvpK1r.png">
<meta property="og:image" content="https://i.imgur.com/Orti8J9.png">
<meta property="og:image" content="https://i.imgur.com/k006QFs.png">
<meta property="og:image" content="https://i.imgur.com/chW6p6H.png">
<meta property="og:image" content="https://i.imgur.com/5AyiP2i.png">
<meta property="og:image" content="https://i.imgur.com/ehen9BU.png">
<meta property="og:image" content="https://i.imgur.com/BR91KWS.png">
<meta property="og:image" content="https://i.imgur.com/EqBTzcH.png">
<meta property="og:image" content="https://i.imgur.com/gGdUowO.png">
<meta property="article:published_time" content="2022-09-09T15:00:24.000Z">
<meta property="article:modified_time" content="2022-11-03T15:18:45.890Z">
<meta property="article:author" content="Xuanlang">
<meta property="article:tag" content="松鼠小筑">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.mathpix.com/snip/images/4PNilhlBG6Ar9npUJvpzHe04Da9TCX2XZTiS9frwkpk.original.fullsize.png">


<link rel="canonical" href="https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/","path":"2022/09/09/Machine-Learning/","title":"Machine Learning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Machine Learning | 松鼠小筑</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?2c6ad69c6dce83b3987864c3d69796db"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">松鼠小筑</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">学习笔记</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">19</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">79</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">Supervised learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-regression"><span class="nav-number">1.1.</span> <span class="nav-text">Linear regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model"><span class="nav-number">1.1.1.</span> <span class="nav-text">model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost-function"><span class="nav-number">1.1.2.</span> <span class="nav-text">Cost function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-descent"><span class="nav-number">1.1.3.</span> <span class="nav-text">Gradient descent</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#algorithm"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiple-Linear-regression"><span class="nav-number">1.2.</span> <span class="nav-text">Multiple Linear regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model"><span class="nav-number">1.2.1.</span> <span class="nav-text">Model:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vectorization"><span class="nav-number">1.2.2.</span> <span class="nav-text">Vectorization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multiple-Gradient-descent"><span class="nav-number">1.2.3.</span> <span class="nav-text">Multiple Gradient descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-scaling"><span class="nav-number">1.2.4.</span> <span class="nav-text">Feature scaling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-regression"><span class="nav-number">1.3.</span> <span class="nav-text">Logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Decision-boundary"><span class="nav-number">1.3.1.</span> <span class="nav-text">Decision boundary</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost-Lost-function"><span class="nav-number">1.3.2.</span> <span class="nav-text">Cost&#x2F;Lost function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#overfitting"><span class="nav-number">1.3.3.</span> <span class="nav-text">overfitting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-decent"><span class="nav-number">1.3.4.</span> <span class="nav-text">Gradient decent</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Decision-tree"><span class="nav-number">1.4.</span> <span class="nav-text">Decision tree</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#use-Entropy-to-measure-purity"><span class="nav-number">1.4.1.</span> <span class="nav-text">use Entropy to measure purity</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Network-Model"><span class="nav-number">2.1.</span> <span class="nav-text">Neural Network Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mutiple-layers-neural-networks"><span class="nav-number">2.2.</span> <span class="nav-text">Mutiple layers neural networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Forward-propagation"><span class="nav-number">2.3.</span> <span class="nav-text">Forward propagation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#specific-calculation"><span class="nav-number">2.3.1.</span> <span class="nav-text">specific calculation:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-a-neural-network"><span class="nav-number">2.4.</span> <span class="nav-text">Training a neural network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Create-model"><span class="nav-number">2.4.1.</span> <span class="nav-text">Create model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-model"><span class="nav-number">2.4.2.</span> <span class="nav-text">Train model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Activation-functions"><span class="nav-number">2.5.</span> <span class="nav-text">Activation functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#How-to-choose"><span class="nav-number">2.5.1.</span> <span class="nav-text">How to choose?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiclass-Classification"><span class="nav-number">2.6.</span> <span class="nav-text">Multiclass Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax-regression"><span class="nav-number">2.6.1.</span> <span class="nav-text">Softmax regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost"><span class="nav-number">2.6.2.</span> <span class="nav-text">Cost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming"><span class="nav-number">2.6.3.</span> <span class="nav-text">Programming</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimization-and-Diagnosing"><span class="nav-number">2.7.</span> <span class="nav-text">Optimization and Diagnosing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam-algorithm"><span class="nav-number">2.7.1.</span> <span class="nav-text">Adam algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Layer-types"><span class="nav-number">2.8.</span> <span class="nav-text">Layer types</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dense-Layer"><span class="nav-number">2.8.1.</span> <span class="nav-text">Dense Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convolutional-Layer"><span class="nav-number">2.8.2.</span> <span class="nav-text">Convolutional Layer</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">Unsupervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#PCA-principle-component-analysis"><span class="nav-number">3.1.</span> <span class="nav-text">PCA(principle component analysis)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#theoretical-part"><span class="nav-number">3.1.1.</span> <span class="nav-text">theoretical part</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#practical-part"><span class="nav-number">3.1.2.</span> <span class="nav-text">practical part</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kmeans"><span class="nav-number">3.2.</span> <span class="nav-text">Kmeans</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">TensorFlow</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Simple-example"><span class="nav-number">4.1.</span> <span class="nav-text">Simple example:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-a-neural-network"><span class="nav-number">4.2.</span> <span class="nav-text">Train a neural network:</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xuanlang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Xuanlang</p>
  <div class="site-description" itemprop="description">业精于勤荒于嬉，行成于思毁于随</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">79</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/1982606762" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;1982606762" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zxl17302206700@gmail.com" title="E-Mail → mailto:zxl17302206700@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Xuanlang">
      <meta itemprop="description" content="业精于勤荒于嬉，行成于思毁于随">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="松鼠小筑">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine Learning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-09-09 17:00:24" itemprop="dateCreated datePublished" datetime="2022-09-09T17:00:24+02:00">2022-09-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-11-03 16:18:45" itemprop="dateModified" datetime="2022-11-03T16:18:45+01:00">2022-11-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Changyan：</span>
    
    <a title="Machine Learning" href="/2022/09/09/Machine-Learning/#SOHUCS" itemprop="discussionUrl">
      <span id="sourceId::8c5e7a4bd91da6775924abcf676ec313" class="cy_cmt_count" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>26 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>A machine-Learning Learning notes</p>
<span id="more"></span>
<h1>Supervised learning</h1>
<h2 id="Linear-regression"><a class="header-anchor" href="#Linear-regression">¶</a>Linear regression</h2>
<h3 id="model"><a class="header-anchor" href="#model">¶</a>model</h3>
<p>$$<br>
f_{\omega,b}(x) = wx+b<br>
$$</p>
<p>change the w and b to optimize the algorithm.</p>
<h3 id="Cost-function"><a class="header-anchor" href="#Cost-function">¶</a>Cost function</h3>
<p>Used to evaluate the model. The goal is to use cost function to minimize it and to find the best parameters for the model.</p>
<p>used for linear regression is<br>
$$<br>
J(\omega,b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{\omega,b}(x^{(i)})-y^{(i)})^2<br>
$$<br>
It’s related with w and b</p>
<p>Graph when J is only related to w:</p>
<p><img data-src="https://cdn.mathpix.com/snip/images/4PNilhlBG6Ar9npUJvpzHe04Da9TCX2XZTiS9frwkpk.original.fullsize.png" alt=""></p>
<h3 id="Gradient-descent"><a class="header-anchor" href="#Gradient-descent">¶</a>Gradient descent</h3>
<p>In a 3D plot which choose w,b as the ground,J(w,b)as the <a target="_blank" rel="noopener" href="http://hight.To">hight.To</a> find the best w,b it needs to go down from a “hill” to a “valley”. You look around and go step by step from the hill untill can’t go down any further, that’s a valley.</p>
<h4 id="algorithm"><a class="header-anchor" href="#algorithm">¶</a>algorithm</h4>
<p>$$<br>
w=w-\alpha \frac{\partial}{\partial w} J(w, b)<br>
$$<br>
$$<br>
b=b-\alpha \frac{\partial}{\partial b} J(w, b)<br>
$$</p>
<p>$\alpha$ means “Learning rate”, which controls how largh you take for each step downhill.</p>
<p>You need to update w and b simultaneously, the correct method are as follows:<br>
$$<br>
\begin{aligned}<br>
&amp;t m p_{-} w=w-\alpha \frac{\partial}{\partial w} J(w, b) \<br>
&amp;{tmp_{-}b=b-\alpha \frac{\partial}{\partial b} J\left(w,b\right)} \<br>
&amp;w=t m p_{-} w\<br>
&amp;b=tmp_{-}b<br>
\end{aligned}<br>
$$<br>
Repeat doing this untile w and b convergence.</p>
<p>Why use derivative?</p>
<p>The learning rate is always positive,so if the derivative is negative, to make the J(w,b) smaller you need to increase w. If the derivative is positive, you need to decreate w.</p>
<p>The cost function usually shape like a bowl, so it only have one minimum point.</p>
<h2 id="Multiple-Linear-regression"><a class="header-anchor" href="#Multiple-Linear-regression">¶</a>Multiple Linear regression</h2>
<p>$\mathrm{x}_j$ = $j^{\text {th }}$ feature</p>
<p>$\overrightarrow{\mathrm{x}}^{(i)}$ = features of $i^{\text {th }}$ training example, this is a vector</p>
<p>$\mathrm{X}_j^{(i)}$ = value of feature j in $i^{\text {th }}$ training example</p>
<h3 id="Model"><a class="header-anchor" href="#Model">¶</a>Model:</h3>
<p>$$<br>
f_{w, b}(x)=w_1 x_1+w_2 x_2+\cdots+w_n x_n+b<br>
$$</p>
<p>$\vec{\omega}=\left[\begin{array}{lllll}w_1 &amp; w_2 &amp; w_3 &amp; \ldots &amp; w_n\end{array}\right]$</p>
<p>It can be rewritten as:</p>
<p>$f_{\overrightarrow{\mathrm{w}}, b}(\overrightarrow{\mathrm{x}})=\overrightarrow{\mathrm{w}} \cdot \overrightarrow{\mathrm{x}}+b$</p>
<h3 id="Vectorization"><a class="header-anchor" href="#Vectorization">¶</a>Vectorization</h3>
<p>overall: use numpy as much as possible to make the program run faster.</p>
<p>Without vectorization you need to use for loop to calculate f</p>
<p>With vectorization you can use np.dot(w,x) to get the result.</p>
<h3 id="Multiple-Gradient-descent"><a class="header-anchor" href="#Multiple-Gradient-descent">¶</a>Multiple Gradient descent</h3>
<p>Repeat{</p>
<p>$\left.w_1=w_1-\alpha \frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right) x_1^{(i)}\right)$</p>
<p>​						=&gt;$\frac{\partial}{\partial w_1} J(\overrightarrow{\mathrm{w}}, b)$</p>
<p>$\vdots$</p>
<p>$w_n=w_n-\alpha \frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right) x_n^{(i)}$</p>
<p>$b=b-\alpha \frac{1}{m} \sum_{i=1}^m\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right)$</p>
<p>simultaneously update<br>
$w_j($ for $j=1, \cdots, n)$ and $b$</p>
<p>}</p>
<h3 id="Feature-scaling"><a class="header-anchor" href="#Feature-scaling">¶</a>Feature scaling</h3>
<p>Choose proper w1,w2…can make gradient descent faster.</p>
<h2 id="Logistic-regression"><a class="header-anchor" href="#Logistic-regression">¶</a>Logistic regression</h2>
<p>use sigmoid function to make classifition.</p>
<p>Sigmoid function:</p>
<p>$g(z)=\frac{1}{1+e^{-z}} \quad 0&lt;g(z)&lt;1$</p>
<p>Logistic regression model:<br>
$$<br>
z=\overrightarrow{\mathrm{w}} \cdot \overrightarrow{\mathrm{x}}+b<br>
\<br>
g(z)=\frac{1}{1+e^{-z}}<br>
$$</p>
<h3 id="Decision-boundary"><a class="header-anchor" href="#Decision-boundary">¶</a>Decision boundary</h3>
<p>choose a threshold to determin whether $\hat y$ is 0 or 1.</p>
<p>Normally use when $z=\overrightarrow{\mathrm{w}} \cdot \overrightarrow{\mathrm{x}}+b=0$ when it’s a linear situation.</p>
<p>When it’s a non-linear situation, use this one $z=x_1^2+x_2^2-1=0$</p>
<h3 id="Cost-Lost-function"><a class="header-anchor" href="#Cost-Lost-function">¶</a>Cost/Lost function</h3>
<p>Previous: $J(\overrightarrow{\mathrm{w}}, b)=\frac{1}{m} \sum_{i=1}^m \frac{1}{2}\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)-y^{(i)}\right)^2$ squared error.</p>
<p>It’s cost function is a non-convex, so need a new function.</p>
<p>new version:<br>
$$<br>
L\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right), y^{(i)}\right)=\left{\begin{aligned}<br>
-\log \left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)\right) &amp; \text { if } y^{(i)}=1 \<br>
-\log \left(1-f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)\right) &amp; \text { if } y^{(i)}=0<br>
\end{aligned}\right.<br>
$$</p>
<p><img data-src="https://i.imgur.com/zRvpK1r.png" alt="image-2022101512523502 AM"></p>
<p>simplified version:<br>
$$<br>
L\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right), y^{(i)}\right)=-y^{(i)} \log \left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)\right)-\left(1-y^{(i)}\right) \log \left(1-f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)\right)<br>
$$<br>
It’s cost function:<br>
$$<br>
J(\mathbf{w}, b)=\frac{1}{m} \sum_{i=0}^{m-1}\left[\operatorname{loss}\left(f_{\mathbf{w}, b}\left(\mathbf{x}^{(i)}\right), y^{(i)}\right)\right]<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_logistic</span>(<span class="params">X, y, w, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes cost</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n)): Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,)) : target values</span></span><br><span class="line"><span class="string">      w (ndarray (n,)) : model parameters  </span></span><br><span class="line"><span class="string">      b (scalar)       : model parameter</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      cost (scalar): cost</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        z_i = np.dot(X[i],w) + b</span><br><span class="line">        f_wb_i = sigmoid(z_i)</span><br><span class="line">        cost +=  -y[i]*np.log(f_wb_i) - (<span class="number">1</span>-y[i])*np.log(<span class="number">1</span>-f_wb_i)</span><br><span class="line">             </span><br><span class="line">    cost = cost / m</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="overfitting"><a class="header-anchor" href="#overfitting">¶</a>overfitting</h3>
<ul>
<li>Underfit:the model doesn’t fit the training set well.(high bias in prediction result)</li>
<li>Overfit : the model fits the training set extremely well.(high variance in result, small change cause huge difference)</li>
</ul>
<p>How to deal with it?</p>
<ul>
<li>collect more training examples</li>
<li>Reduce features to use</li>
<li>reduce the size of parameters(regularization)</li>
</ul>
<h3 id="Gradient-decent"><a class="header-anchor" href="#Gradient-decent">¶</a>Gradient decent</h3>
<p>repeat until convergence: {<br>
$$<br>
\begin{aligned}<br>
&amp;b:=b-\alpha \frac{\partial J(\mathbf{w}, b)}{\partial b} \<br>
&amp;w_j:=w_j-\alpha \frac{\partial J(\mathbf{w}, b)}{\partial w_j} \quad \text { for } \mathrm{j}:=0 . . \mathrm{n}-1<br>
\end{aligned}<br>
$$<br>
}<br>
$$<br>
\begin{gathered}<br>
\frac{\partial J(\mathbf{w}, b)}{\partial b}=\frac{1}{m} \sum_{i=0}^{m-1}\left(f_{\mathbf{w}, b}\left(\mathbf{x}^{(i)}\right)-\mathbf{y}^{(i)}\right) \<br>
\frac{\partial J(\mathbf{w}, b)}{\partial w_j}=\frac{1}{m} \sum_{i=0}^{m-1}\left(f_{\mathbf{w}, b}\left(\mathbf{x}^{(i)}\right)-\mathbf{y}^{(i)}\right) x_j^{(i)}<br>
\end{gathered}<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_gradient_logistic</span>(<span class="params">X, y, w, b</span>):</span> </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the gradient for linear regression </span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n): Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,)): target values</span></span><br><span class="line"><span class="string">      w (ndarray (n,)): model parameters  </span></span><br><span class="line"><span class="string">      b (scalar)      : model parameter</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. </span></span><br><span class="line"><span class="string">      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    dj_dw = np.zeros((n,))                           <span class="comment">#(n,)</span></span><br><span class="line">    dj_db = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb_i = sigmoid(np.dot(X[i],w) + b)          <span class="comment">#(n,)(n,)=scalar</span></span><br><span class="line">        err_i  = f_wb_i  - y[i]                       <span class="comment">#scalar</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      <span class="comment">#scalar</span></span><br><span class="line">        dj_db = dj_db + err_i</span><br><span class="line">    dj_dw = dj_dw/m                                   <span class="comment">#(n,)</span></span><br><span class="line">    dj_db = dj_db/m                                   <span class="comment">#scalar</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dj_db, dj_dw  </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_</span>):</span> </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Performs batch gradient descent to learn theta. Updates theta by taking </span></span><br><span class="line"><span class="string">    num_iters gradient steps with learning rate alpha</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X :    (array_like Shape (m, n)</span></span><br><span class="line"><span class="string">      y :    (array_like Shape (m,))</span></span><br><span class="line"><span class="string">      w_in : (array_like Shape (n,))  Initial values of parameters of the model</span></span><br><span class="line"><span class="string">      b_in : (scalar)                 Initial value of parameter of the model</span></span><br><span class="line"><span class="string">      cost_function:                  function to compute cost</span></span><br><span class="line"><span class="string">      alpha : (float)                 Learning rate</span></span><br><span class="line"><span class="string">      num_iters : (int)               number of iterations to run gradient descent</span></span><br><span class="line"><span class="string">      lambda_ (scalar, float)         regularization constant</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      w : (array_like Shape (n,)) Updated values of parameters of the model after</span></span><br><span class="line"><span class="string">          running gradient descent</span></span><br><span class="line"><span class="string">      b : (scalar)                Updated value of parameter of the model after</span></span><br><span class="line"><span class="string">          running gradient descent</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># number of training examples</span></span><br><span class="line">    m = <span class="built_in">len</span>(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># An array to store cost J and w&#x27;s at each iteration primarily for graphing later</span></span><br><span class="line">    J_history = []</span><br><span class="line">    w_history = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the gradient and update the parameters</span></span><br><span class="line">        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update Parameters using w, b, alpha and gradient</span></span><br><span class="line">        w_in = w_in - alpha * dj_dw               </span><br><span class="line">        b_in = b_in - alpha * dj_db              </span><br><span class="line">       </span><br><span class="line">        <span class="comment"># Save cost J at each iteration</span></span><br><span class="line">        <span class="keyword">if</span> i&lt;<span class="number">100000</span>:      <span class="comment"># prevent resource exhaustion </span></span><br><span class="line">            cost =  cost_function(X, y, w_in, b_in, lambda_)</span><br><span class="line">            J_history.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span></span><br><span class="line">        <span class="keyword">if</span> i% math.ceil(num_iters/<span class="number">10</span>) == <span class="number">0</span> <span class="keyword">or</span> i == (num_iters-<span class="number">1</span>):</span><br><span class="line">            w_history.append(w_in)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Iteration <span class="subst">&#123;i:<span class="number">4</span>&#125;</span>: Cost <span class="subst">&#123;<span class="built_in">float</span>(J_history[-<span class="number">1</span>]):<span class="number">8.2</span>f&#125;</span>   &quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> w_in, b_in, J_history, w_history <span class="comment">#return w and J,w history for graphing</span></span><br></pre></td></tr></table></figure>
<h2 id="Decision-tree"><a class="header-anchor" href="#Decision-tree">¶</a>Decision tree</h2>
<p>A predict which features are discrete values.</p>
<p>Questions about the algorithm:</p>
<ol>
<li>How to choose parameters?
<ol>
<li>Maximize purity</li>
</ol>
</li>
<li>When to stop splitting
<ol>
<li>When a node is 100% one class</li>
<li>When splitting a node will result in the tree exceeding a maximum depth</li>
<li>When improvements in purity score are below a threshold</li>
</ol>
</li>
</ol>
<h3 id="use-Entropy-to-measure-purity"><a class="header-anchor" href="#use-Entropy-to-measure-purity">¶</a>use Entropy to measure purity</h3>
<p><img data-src="https://i.imgur.com/Orti8J9.png" alt="image-2022110314327111 PM"></p>
<p>$\begin{aligned} H\left(p_1\right) &amp;=-p_1 \log _2\left(p_1\right)-p_0 \log _2\left(p_0\right) \ &amp;=-p_1 \log _2\left(p_1\right)-\left(1-p_1\right) \log _2\left(1-p_1\right) \end{aligned}$</p>
<h1>Neural Networks</h1>
<h2 id="Neural-Network-Model"><a class="header-anchor" href="#Neural-Network-Model">¶</a>Neural Network Model</h2>
<p>activation function:$a=f(x)=\frac{1}{1+e^{-(w x+b)}}$</p>
<p>If a vector X begin input into the first layer which consists three neurons.</p>
<p>Each neuron will return $a_1=g(\overrightarrow{\mathrm{w}} \cdot \overrightarrow{\mathrm{x}}+b)*weight$ and composits a vector of three numbers, which is the output of this layer.</p>
<p>$w_1^{\text {[1] }}$ denotes the first layer’s  first w.</p>
<h2 id="Mutiple-layers-neural-networks"><a class="header-anchor" href="#Mutiple-layers-neural-networks">¶</a>Mutiple layers neural networks</h2>
<p>Notation:</p>
<p>$a_j^{[l]}=g\left(\vec{w}_j^{[l]} \cdot \vec{a}^{[l-1]}+b_j^{[l]}\right)$</p>
<p><img data-src="https://i.imgur.com/k006QFs.png" alt="image-20221018111702509 AM"></p>
<h2 id="Forward-propagation"><a class="header-anchor" href="#Forward-propagation">¶</a>Forward propagation</h2>
<p><img data-src="https://i.imgur.com/chW6p6H.png" alt="image-2022101815035177 PM"></p>
<h3 id="specific-calculation"><a class="header-anchor" href="#specific-calculation">¶</a>specific calculation:</h3>
<p>$x=n p \cdot \operatorname{array}([200,17])$</p>
<p>$a_1^{[1]}=g\left(\overrightarrow{\mathrm{w}}_1^{[1]} \cdot \overrightarrow{\mathrm{x}}+b_1^{[1]}\right)$</p>
<p>w1_1 $=$ np. array $([1,2])$<br>
b1_1 $=$ np. $\operatorname{array}([-1])$<br>
$z 11=n p \cdot \operatorname{dot}(w 11, x)+b 11$<br>
a1_1 = sigmoid $\left(z 1_{-1}\right)$</p>
<p>and then a1 = np. array( [a1_1,a1_2,a1_3]</p>
<h2 id="Training-a-neural-network"><a class="header-anchor" href="#Training-a-neural-network">¶</a>Training a neural network</h2>
<ol>
<li>specify how to compute output</li>
<li>specify loss and cost</li>
<li>Train to minimize lost</li>
</ol>
<h3 id="Create-model"><a class="header-anchor" href="#Create-model">¶</a>Create model</h3>
<p>use sequence and dense func</p>
<h3 id="Train-model"><a class="header-anchor" href="#Train-model">¶</a>Train model</h3>
<p>repeat {<br>
$$<br>
\begin{aligned}<br>
&amp;w_j^{[l]}=w_j^{[l]}-\alpha \frac{\partial}{\partial w_j} J(\overrightarrow{\mathrm{w}}, b) \<br>
&amp;b_j^{[l]}=b_j^{[l]}-\alpha \frac{\partial}{\partial b j} J(\overrightarrow{\mathrm{w}}, b)<br>
\end{aligned}<br>
$$<br>
$$<br>
\text { } }<br>
$$</p>
<p><code>model.fit(X,y,epochs=100)</code></p>
<h2 id="Activation-functions"><a class="header-anchor" href="#Activation-functions">¶</a>Activation functions</h2>
<p>Linear activation function $g(z)=z$</p>
<p>Sigmoid $g(z)=\frac{1}{1+e^{-z}}$</p>
<p>ReLU $g(z)=\max (0, z)$</p>
<p><img data-src="https://i.imgur.com/5AyiP2i.png" alt="image-2022101985528546 PM"></p>
<h3 id="How-to-choose"><a class="header-anchor" href="#How-to-choose">¶</a>How to choose?</h3>
<p>output layer:</p>
<p>Binary cassification: Signoid</p>
<p>Regression:Linear activation</p>
<p>Regression with all positive:ReLU</p>
<p>Hidden layer</p>
<p>in hidden layer mostly ReLU</p>
<h2 id="Multiclass-Classification"><a class="header-anchor" href="#Multiclass-Classification">¶</a>Multiclass Classification</h2>
<h3 id="Softmax-regression"><a class="header-anchor" href="#Softmax-regression">¶</a>Softmax regression</h3>
<p>$z_1=\overrightarrow{\mathrm{w}}_1 \cdot \overrightarrow{\mathrm{x}}+b_1$</p>
<p>$a_1=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}$<br>
$$<br>
z_j=\overrightarrow{\mathrm{w}}_j \cdot \overrightarrow{\mathrm{x}}+b_j \quad \mathrm{j}=1, \ldots, \mathrm{N}<br>
$$</p>
<p>$$<br>
a_j=\frac{e^{z_j}}{\sum_{k=1}^N e^{z_k}}=\mathrm{P}(\mathrm{y}=j \mid \overrightarrow{\mathrm{x}})<br>
$$</p>
<h3 id="Cost"><a class="header-anchor" href="#Cost">¶</a>Cost</h3>
<p>$$<br>
a_N=\frac{e^{z_N}}{e^{z_1}+e^{z_2}+\cdots+e^{z_N}}=P(y=N \mid \overrightarrow{\mathrm{x}})<br>
$$</p>
<p>$$<br>
\operatorname{loss}\left(a_1, \ldots, a_N, y\right)=\left{\begin{array}{lc}<br>
-\log a_1 &amp; \text { if } y=1 \<br>
-\log a_2 &amp; \text { if } y=2 \<br>
&amp; \vdots \<br>
-\log a_N &amp; \text { if } y=N<br>
\end{array}\right.<br>
$$</p>
<h3 id="Programming"><a class="header-anchor" href="#Programming">¶</a>Programming</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Dense(units=<span class="number">10</span>,activation=<span class="string">&#x27;softmax&#x27;</span>)<span class="comment">#output layer</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=SparseCategoricalCrossentropy())</span><br></pre></td></tr></table></figure>
<p>Better way(recommended):</p>
<p>To make it more numerically accurate.</p>
<p>From $\operatorname{loss}=-y \log (a)-(1-y) \log (1-a)$</p>
<p>to $\operatorname{loss}=-y \log \left(\frac{1}{1+e^{-z}}\right)-(1-y) \log \left(1-\frac{1}{1+e^{-z}}\right)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Dense(units=<span class="number">10</span>,activation=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=SparseCategoricalCrossEntropy(from_logits=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<h2 id="Optimization-and-Diagnosing"><a class="header-anchor" href="#Optimization-and-Diagnosing">¶</a>Optimization and Diagnosing</h2>
<h3 id="Adam-algorithm"><a class="header-anchor" href="#Adam-algorithm">¶</a>Adam algorithm</h3>
<p>Adaptive Moment estimation</p>
<p>automatically change alpha while doing learning.Has a lot of alpha for different parameters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">compile</span>(tf.keras.optimizers.Adam(learning_rate=<span class="number">1e-3</span>))</span><br></pre></td></tr></table></figure>
<h2 id="Layer-types"><a class="header-anchor" href="#Layer-types">¶</a>Layer types</h2>
<h3 id="Dense-Layer"><a class="header-anchor" href="#Dense-Layer">¶</a>Dense Layer</h3>
<p>output is a function of all the activation function result</p>
<h3 id="Convolutional-Layer"><a class="header-anchor" href="#Convolutional-Layer">¶</a>Convolutional Layer</h3>
<p>Each neuron only looks at part of the previous layer’s output</p>
<h1>Unsupervised Learning</h1>
<h2 id="PCA-principle-component-analysis"><a class="header-anchor" href="#PCA-principle-component-analysis">¶</a>PCA(principle component analysis)</h2>
<h3 id="theoretical-part"><a class="header-anchor" href="#theoretical-part">¶</a>theoretical part</h3>
<p>It’s propose is to reduce dimension of input data. Suppose we have sample X with n dimensions $X=\left{x_0, x_1, \ldots, x_m\right}$ and we want to have a transformation $y=P x$ in which P is a matrix. Then we get some data Y with k dimensions $Y=\left{y_0, y_1, \ldots, y_m\right}$ .</p>
<ol>
<li>
<p>preprocessing data</p>
<p>normolize input data by subtracting their mean of each column.</p>
</li>
<li>
<p>Do the PCA as follows:</p>
</li>
</ol>
<p>we need to make sure these two laws of PCA:</p>
<ul>
<li>After reducing, each of the dimensions should be independent, which means every axis are orthogonal after PCA.</li>
<li>maximize the variance of each dimensions, which means keeping the original data as much as possible.</li>
</ul>
<p>First we can denote the covariance matrix after transportation as $B_{k \times k}=\frac{1}{m} Y Y^T$ , to make each dimensions independent B should be a diagonal matrix, which means it’s all 0 except for it’s diagonal.</p>
<p>Let’s substitute this equation $y=P x$  into the equation $B_{k \times k}=\frac{1}{m} Y Y^T$ and get:<br>
$$<br>
B_{k \times k}=\frac{1}{m} Y Y^T=\frac{1}{m} P X(P X)^T=P \frac{1}{m} X X^T P^T=P_{k \times n} C_{n \times n} P_{n \times k}^T<br>
$$<br>
in which $C_{n \times n}=\frac{1}{m} X X^T$ is the covariance matrix of training data.</p>
<p>Use eigenvalue decomposition on C and get $D_{n \times n}=Q_{n \times n} C_{n \times n} Q_{n \times n}^T$ and D is a diagonal matrix.</p>
<p>So we can know P is a matrix composed by k-th big eigenvectors in line. Each item in B is the eigenvalues sorted in descending order. Eigenvalues shows the degree of confidence of each eigenvectors.</p>
<h3 id="practical-part"><a class="header-anchor" href="#practical-part">¶</a>practical part</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="literal">None</span>, copy=<span class="literal">True</span>, whiten=<span class="literal">False</span>)</span><br><span class="line">pca.fit(X)</span><br></pre></td></tr></table></figure>
<p>parameters:</p>
<blockquote>
<p>n_ components : the component number needs to be kept in the result.</p>
<p>​							None: keep all components<br>
​							int : number of components<br>
​							String: choose components automatically</p>
<p>copy: whether keep the original data</p>
<p>​			true : original data keep same<br>
​			false: original data changes into lower dimension</p>
<p>whiten: if whiten the data</p>
</blockquote>
<p>pca’s parameters :</p>
<blockquote>
<p>components_ : return those includes max variance.</p>
<p>explained_variance_ratio : return their variance’s percentage</p>
<p>n_ components_ :return the number of components</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA()</span><br><span class="line">pca.fit(normed)</span><br><span class="line">x = pca.components_</span><br><span class="line">pca_2_compo = pca.components_[<span class="number">0</span>:<span class="number">2</span>, :]</span><br><span class="line">mapped_input = np.dot(normed, pca_2_compo.T)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">explained_variance = pca.explained_variance_ratio_</span><br><span class="line">top_10 = np.<span class="built_in">sum</span>(explained_variance[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of components&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;explained variance&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Explained Variance&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>,<span class="number">65</span>), np.log(pca.explained_variance_ratio_), <span class="string">&#x27;r--&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of components&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;eigenspectrum&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Eigenspectrum&#x27;</span>)</span><br><span class="line"></span><br><span class="line">top5 = pca.components_[:<span class="number">5</span>]</span><br><span class="line">top5 = top5.reshape(<span class="number">5</span>,<span class="number">8</span>,<span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">5</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(top5[i], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img data-src="https://i.imgur.com/ehen9BU.png" alt="image-2022101813109545 AM"></p>
<p><img data-src="https://i.imgur.com/BR91KWS.png" alt="image-2022101813118956 AM"></p>
<p><img data-src="https://i.imgur.com/EqBTzcH.png" alt="image-2022101813125171 AM"></p>
<h2 id="Kmeans"><a class="header-anchor" href="#Kmeans">¶</a>Kmeans</h2>
<p>used for clustering.</p>
<h1>TensorFlow</h1>
<h2 id="Simple-example"><a class="header-anchor" href="#Simple-example">¶</a>Simple example:</h2>
<p><img data-src="https://i.imgur.com/gGdUowO.png" alt="image-2022101835321992 PM"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">200.0</span>,<span class="number">17.0</span>]])</span><br><span class="line">layer_1 = Dense(units=<span class="number">3</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">a1 = layer_1(x)</span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">a2 = layer_2(a1)</span><br><span class="line"><span class="keyword">if</span> a2 &gt;= <span class="number">0.5</span>:</span><br><span class="line">  yhat = <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  yhat = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h2 id="Train-a-neural-network"><a class="header-anchor" href="#Train-a-neural-network">¶</a>Train a neural network:</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layer_1 = Dense(units=<span class="number">3</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">layer_2 = Dense(units=<span class="number">1</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">model = Sequential([layer_1,layer_2])</span><br><span class="line">model.<span class="built_in">compile</span>(...)</span><br><span class="line">model.fit(x,y)<span class="comment">#x is data, y is label</span></span><br><span class="line">model.predict(x_new)<span class="comment"># get the new data </span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Or in a simple way:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential([</span><br><span class="line">  Dense(units=<span class="number">3</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">  Dense(units=<span class="number">1</span>,activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>in compile we need to specify loss function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line"> loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    optimizer=tf.keras.optimizers.Adam(<span class="number">0.01</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Xuanlang
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/" title="Machine Learning">https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/08/16/conda%E7%9B%B8%E5%85%B3%E4%BD%BF%E7%94%A8/" rel="prev" title="conda相关使用">
                  <i class="fa fa-chevron-left"></i> conda相关使用
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/09/13/MLA-assignment-1/" rel="next" title="MLA-assignment-1">
                  MLA-assignment-1 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="SOHUCS" sid="8c5e7a4bd91da6775924abcf676ec313"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">津ICP备19008018号-1 </a>
  </div>

<div class="copyright">
  &copy; 2019 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-award"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xuanlang</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/lozad@1.16.0/dist/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/quicklink@2.2.0/dist/quicklink.umd.js" integrity="sha256-4kQf9z5ntdQrzsBC3YSHnEz02Z9C1UeW/E9OgnvlzSY=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://www.zhaoxuanlang.cn/2022/09/09/Machine-Learning/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script class="next-config" data-name="changyan" type="application/json">{"enable":true,"appid":"cyvRiUJ7H","appkey":"bfa85e46b2e4cf46d837cbd6e78ba7ab"}</script>
<script src="/js/third-party/comments/changyan.js"></script>

</body>
</html>

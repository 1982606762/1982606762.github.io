<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">

<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/32*32.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/32*32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/16*16.ico">
  <link rel="mask-icon" href="/images/32*32.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.zhaoxuanlang.cn","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":true},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"manual"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Learning note for the course Deep Learning Specialization">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep-Learning">
<meta property="og:url" content="https://www.zhaoxuanlang.cn/2024/07/20/Deep-Learning/index.html">
<meta property="og:site_name" content="松鼠小筑">
<meta property="og:description" content="Learning note for the course Deep Learning Specialization">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-07-19T23:30:32.000Z">
<meta property="article:modified_time" content="2024-09-09T11:08:57.754Z">
<meta property="article:author" content="Xuanlang">
<meta property="article:tag" content="松鼠小筑">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://www.zhaoxuanlang.cn/2024/07/20/Deep-Learning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.zhaoxuanlang.cn/2024/07/20/Deep-Learning/","path":"2024/07/20/Deep-Learning/","title":"Deep-Learning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Deep-Learning | 松鼠小筑</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?2c6ad69c6dce83b3987864c3d69796db"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">松鼠小筑</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">学习笔记</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">33</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">13</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">104</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#neural-networks-and-deep-learning"><span class="nav-number">1.</span> <span class="nav-text">Neural Networks and Deep
Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction-to-deep-learning"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction to Deep
Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#notations"><span class="nav-number">1.2.</span> <span class="nav-text">Notations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic-regression-as-a-neural-network"><span class="nav-number">1.3.</span> <span class="nav-text">Logistic Regression as
a Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#defination"><span class="nav-number">1.3.1.</span> <span class="nav-text">Defination</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cost-function"><span class="nav-number">1.3.2.</span> <span class="nav-text">Cost function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-descent"><span class="nav-number">1.3.3.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vectorization"><span class="nav-number">1.3.4.</span> <span class="nav-text">Vectorization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shallow-neural-networks"><span class="nav-number">1.4.</span> <span class="nav-text">Shallow Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#neural-network-representation"><span class="nav-number">1.4.1.</span> <span class="nav-text">Neural Network
Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%92%88%E5%AF%B9%E5%8D%95%E4%B8%80%E6%A1%88%E4%BE%8B%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83"><span class="nav-number">1.4.2.</span> <span class="nav-text">针对单一案例的神经网络训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%92%88%E5%AF%B9%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%85%A5%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.4.3.</span> <span class="nav-text">针对多个输入的神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#activation-functions"><span class="nav-number">1.4.4.</span> <span class="nav-text">Activation functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.4.5.</span> <span class="nav-text">神经网络的反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">1.4.6.</span> <span class="nav-text">计算梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E7%BD%B2"><span class="nav-number">1.4.7.</span> <span class="nav-text">代码部署</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0"><span class="nav-number">1.4.7.1.</span> <span class="nav-text">初始化参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.4.7.2.</span> <span class="nav-text">正向传播：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.4.7.3.</span> <span class="nav-text">计算损失函数：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADgrad_summary"><span class="nav-number">1.4.7.4.</span> <span class="nav-text">部署反向传播：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0"><span class="nav-number">1.4.7.5.</span> <span class="nav-text">更新参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%84%E5%90%88%E4%BB%A3%E7%A0%81"><span class="nav-number">1.4.7.6.</span> <span class="nav-text">组合代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><span class="nav-number">1.4.7.7.</span> <span class="nav-text">使用模型进行预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-neural-network"><span class="nav-number">1.5.</span> <span class="nav-text">Deep Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9F%A9%E9%98%B5%E5%BD%A2%E7%8A%B6"><span class="nav-number">1.5.1.</span> <span class="nav-text">神经网络中矩阵形状</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E6%95%88%E7%8E%87%E9%AB%98"><span class="nav-number">1.5.2.</span> <span class="nav-text">为什么深度网络效率高？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%90%91%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.5.3.</span> <span class="nav-text">深度神经网络中的正向和反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="nav-number">1.5.3.1.</span> <span class="nav-text">理论基础</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.5.3.2.</span> <span class="nav-text">具体实现：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parameters-v.-hyperparameters"><span class="nav-number">1.5.4.</span> <span class="nav-text">Parameters v.
Hyperparameters</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#improving-deep-neural-networks"><span class="nav-number">2.</span> <span class="nav-text">Improving Deep Neural
Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#practical-aspects-of-deep-learning"><span class="nav-number">2.1.</span> <span class="nav-text">Practical Aspects of Deep
Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#setting-up-machine-learning-application"><span class="nav-number">2.1.1.</span> <span class="nav-text">Setting up Machine
Learning Application</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#biasvariance"><span class="nav-number">2.1.2.</span> <span class="nav-text">Bias&#x2F;Variance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#regularization%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">2.1.3.</span> <span class="nav-text">Regularization正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dropout-redularization"><span class="nav-number">2.1.4.</span> <span class="nav-text">Dropout Redularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#data-augmentation"><span class="nav-number">2.1.5.</span> <span class="nav-text">Data augmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#early-stopping"><span class="nav-number">2.1.6.</span> <span class="nav-text">Early stopping</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="nav-number">2.1.7.</span> <span class="nav-text">归一化输入数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">2.1.8.</span> <span class="nav-text">梯度消失&#x2F;梯度爆炸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-checking"><span class="nav-number">2.1.9.</span> <span class="nav-text">Gradient checking</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#optimization-algorithms"><span class="nav-number">2.2.</span> <span class="nav-text">Optimization Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mini-batch-gradient-descent"><span class="nav-number">2.2.1.</span> <span class="nav-text">Mini-batch gradient descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#exponentially-weighted-average%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E6%95%B0"><span class="nav-number">2.2.2.</span> <span class="nav-text">Exponentially
weighted average指数加权平均数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-descent-with-momentum"><span class="nav-number">2.2.3.</span> <span class="nav-text">Gradient Descent with
momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rmsprop"><span class="nav-number">2.2.4.</span> <span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adam-optimization-algorithm"><span class="nav-number">2.2.5.</span> <span class="nav-text">Adam Optimization Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#learning-rate-decay"><span class="nav-number">2.2.6.</span> <span class="nav-text">Learning Rate Decay</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hyperparameter-tuning"><span class="nav-number">2.3.</span> <span class="nav-text">Hyperparameter Tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#batch-normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">2.3.1.</span> <span class="nav-text">Batch Normalization
批量归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax-multi-class-classification-regression"><span class="nav-number">2.3.2.</span> <span class="nav-text">Softmax
multi-class classification Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#intro-to-tensorflow"><span class="nav-number">2.3.3.</span> <span class="nav-text">Intro to Tensorflow</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xuanlang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Xuanlang</p>
  <div class="site-description" itemprop="description">业精于勤荒于嬉，行成于思毁于随</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">104</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/1982606762" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;1982606762" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zxl17302206700@gmail.com" title="E-Mail → mailto:zxl17302206700@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/xuanlang/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;xuanlang&#x2F;" rel="noopener" target="_blank"><i class="fa fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.zhaoxuanlang.cn/2024/07/20/Deep-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Xuanlang">
      <meta itemprop="description" content="业精于勤荒于嬉，行成于思毁于随">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="松鼠小筑">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deep-Learning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-07-20 01:30:32" itemprop="dateCreated datePublished" datetime="2024-07-20T01:30:32+02:00">2024-07-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-09-09 13:08:57" itemprop="dateModified" datetime="2024-09-09T13:08:57+02:00">2024-09-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2024/07/20/Deep-Learning/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2024/07/20/Deep-Learning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>19k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>35 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>Learning note for the course <a
target="_blank" rel="noopener" href="https://www.coursera.org/specializations/deep-learning">Deep
Learning Specialization</a></p>
<span id="more"></span>
<h1 id="neural-networks-and-deep-learning">Neural Networks and Deep
Learning</h1>
<h2 id="introduction-to-deep-learning">Introduction to Deep
Learning</h2>
<p>Neural network:</p>
<p>有一个输入x，经过一些神经元后计算得到结果y。所以线性回归是一个神经元的神经网络。</p>
<p>复杂模型如预测房价的时候可以有多个神经元</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024081012845398%E2%80%AFPM.png"
alt="image-2024081012845398 PM" />
<figcaption aria-hidden="true">image-2024081012845398 PM</figcaption>
</figure>
<p>Structured Data：如存在数据库里的数据</p>
<p>Unstructured Data： 如图片，音频，视频</p>
<p>要想提升预测的准确性，要不就使用更多数据，要不就使用更大的神经网络模型。</p>
<h2 id="notations">Notations</h2>
<p><strong>General comments:</strong></p>
<ul>
<li>superscript <span class="math inline">\((i)\)</span> will denote the
<span class="math inline">\(i^{\text{th}}\)</span> training example
while superscript <span class="math inline">\([l]\)</span> will denote
the <span class="math inline">\(l^{\text{th}}\)</span> layer</li>
</ul>
<p><strong>Sizes:</strong></p>
<ul>
<li><p><span class="math inline">\(m\)</span>: number of examples in the
dataset</p></li>
<li><p><span class="math inline">\(n_x\)</span>: input size</p></li>
<li><p><span class="math inline">\(n_y\)</span>: output size (or number
of classes)</p></li>
<li><p><span class="math inline">\(n_h^{[l]}\)</span>: number of hidden
units of the <span class="math inline">\(l^{\text{th}}\)</span>
layer</p>
<p>In a for loop, it is possible to denote <span
class="math inline">\(n_x = n_h^{[0]}\)</span> and <span
class="math inline">\(n_y = n_h^{[\text{number of layers} +
1]}\)</span>.</p></li>
<li><p><span class="math inline">\(L\)</span>: number of layers in the
network.</p></li>
</ul>
<p><strong>Objects:</strong></p>
<ul>
<li><span class="math inline">\(X \in \mathbb{R}^{n_x \times m}\)</span>
is the input matrix</li>
<li><span class="math inline">\(x^{(i)} \in \mathbb{R}^{n_x}\)</span> is
the <span class="math inline">\(i^{\text{th}}\)</span> example
represented as a column vector</li>
<li><span class="math inline">\(Y \in \mathbb{R}^{n_y \times m}\)</span>
is the label matrix</li>
<li><span class="math inline">\(y^{(i)} \in \mathbb{R}^{n_y}\)</span> is
the output label for the <span
class="math inline">\(i^{\text{th}}\)</span> example</li>
<li><span class="math inline">\(W^{[l]} \in \mathbb{R}^{\text{number of
units in next layer} \times \text{number of units in the previous
layer}}\)</span> is the weight matrix, superscript <span
class="math inline">\([l]\)</span> indicates the layer</li>
<li><span class="math inline">\(b^{[l]} \in \mathbb{R}^{\text{number of
units in next layer}}\)</span> is the bias vector in the <span
class="math inline">\(l^{\text{th}}\)</span> layer</li>
<li><span class="math inline">\(\hat{y} \in \mathbb{R}^{n_y}\)</span> is
the predicted output vector. It can also be denoted <span
class="math inline">\(a^{[L]}\)</span> where <span
class="math inline">\(L\)</span> is the number of layers in the
network.</li>
</ul>
<p><strong>Common forward propagation equation examples:</strong></p>
<ul>
<li><span class="math inline">\(a = g^{[l]}(W^{[l]}x^{(i)} + b_1) =
g^{[l]}(z_1)\)</span> where <span class="math inline">\(g^{[l]}\)</span>
denotes the <span class="math inline">\(l^{\text{th}}\)</span> layer
activation function</li>
<li><span class="math inline">\(\hat{y}^{(i)} = \text{softmax}(W_{hh} +
b_2)\)</span></li>
</ul>
<p><strong>General Activation Formula:</strong></p>
<ul>
<li><p><span class="math inline">\(a_j^{[l]} = g^{[l]}\left(\sum_k
w_{jk}^{[l]} a_k^{[l-1]} + b_j^{[l]}\right) =
g^{[l]}(z_j^{[l]})\)</span></p></li>
<li><p><span class="math inline">\(J(x, W, b, y)\)</span> or <span
class="math inline">\(J(\hat{y}, y)\)</span> denote the cost
function.</p></li>
</ul>
<p><strong>Examples of cost function:</strong></p>
<ul>
<li><span class="math inline">\(J_{\text{CE}}(\hat{y}, y) =
-\sum_{i=0}^m y^{(i)} \log \hat{y}^{(i)}\)</span></li>
<li><span class="math inline">\(J_1(\hat{y}, y) = \sum_{i=0}^m | y^{(i)}
- \hat{y}^{(i)} |\)</span></li>
</ul>
<h2 id="logistic-regression-as-a-neural-network">Logistic Regression as
a Neural Network</h2>
<h3 id="defination">Defination</h3>
<p>例：给一个二元分类问题：分辨图片里是不是猫</p>
<p>输入是一个<span
class="math inline">\(n_x\)</span>维数组，即图片的像素值。输出<span
class="math inline">\(\hat y\)</span> 即y的预测值。<span
class="math inline">\(z = w^Tx + b\)</span></p>
<p>对于二分问题， 输出<span class="math inline">\(\hat{y} = w^Tx +
b\)</span>,但是他会返回超出0-1范围的值，这时我们就需要加一个sigmoid函数来约束输出结果的范围，这就是逻辑回归。</p>
<p><span class="math inline">\(\hat{y} = \sigma(w^Tx + b)\)</span></p>
<p><span class="math inline">\(\sigma(z) = 1/(1+e^{-z})\)</span></p>
<h3 id="cost-function">Cost function</h3>
<p>然后需要找到w和b的值，w是一个nx维数组，b是一个值。</p>
<p>损失函数<span class="math inline">\(l(\hat
y,y)\)</span>接收预测值和真实值，在逻辑回归中使用下边这个损失函数。</p>
<p><span class="math inline">\(L(\hat y,y) = -(y\log \hat y + (1-y)\log
(1-\hat y))\)</span></p>
<p>Loss function computes the error for a single training example; the
cost function is the average of the loss functions of the entire
training set.</p>
<p>有了loss function后就可以定义cost function <span
class="math inline">\(J(w,b) = \frac{1}{m}\sum_{i=1}^m L(\hat
y^{(i)},y^{(i)})\)</span></p>
<p>用这个函数可以获得最优的w和b。</p>
<h3 id="gradient-descent">Gradient Descent</h3>
<p>有了cost
function后需要找到可以把它最小化的w和b参数。这里我们可以使用梯度下降法。</p>
<p>我们使用的cost function是convex的，所以只有一个全局最小值。</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-20240818121101578%E2%80%AFPM.png"
alt="image-20240818121101578 PM" />
<figcaption aria-hidden="true">image-20240818121101578 PM</figcaption>
</figure>
<p>具体做法：定义一个learning rate <span
class="math inline">\(\alpha\)</span> ,然后重复计算。</p>
<p><strong>Cost Function:</strong></p>
<p><span class="math inline">\(J(\omega, b)\)</span></p>
<p><strong>Gradient Descent Updates:</strong></p>
<p><span class="math inline">\(\omega := \omega - \alpha \frac{\partial
J(\omega, b)}{\partial \omega}\)</span></p>
<p><span class="math inline">\(b := b - \alpha \frac{\partial J(\omega,
b)}{\partial b}\)</span></p>
<p>这里<span
class="math inline">\(\frac{dJ(w)}{dw}\)</span>是J在x=w点处的斜率，即导数。有两个变量的时候就算偏导数。</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-20240818121541768%E2%80%AFPM.png"
alt="image-20240818121541768 PM" />
<figcaption aria-hidden="true">image-20240818121541768 PM</figcaption>
</figure>
<p>对于单个输入的情况我们有</p>
<p><span class="math inline">\(z = w^T x + b\)</span></p>
<p><span class="math inline">\(\hat{y} = a = \sigma(z)\)</span></p>
<p><span class="math inline">\(\mathcal{L}(a, y) = -(y \log(a) + (1 - y)
\log(1 - a))\)</span></p>
<p><span class="math inline">\(\sigma(z) = 1/(1+e^{-z})\)</span></p>
<p>在得到L后现在要反向计算w和b的值</p>
<p>假设输入有两个特征x1和x2，那么有下图：</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024081933341745%E2%80%AFPM.png"
alt="image-2024081933341745 PM" />
<figcaption aria-hidden="true">image-2024081933341745 PM</figcaption>
</figure>
<p>然后计算导数da，给L求a的偏导得到<span
class="math inline">\(\frac{dL(a,y)}{da} = -\frac{y}{a} +
\frac{1-y}{1-a}\)</span></p>
<p>然后计算<span class="math inline">\(dz = \frac{dL}{da}*\frac{da}{dz}
= a-y\)</span></p>
<p>这里<span
class="math inline">\(\frac{da}{dz}\)</span>是sigmoid函数的导数，计算方法如下：</p>
<blockquote>
<p>The sigmoid function is defined as:</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]</span></p>
<p>To find the derivative of ((x)) with respect to (x), we apply the
chain rule. Let:</p>
<p><span class="math display">\[
y = \sigma(x) = \frac{1}{1 + e^{-x}}
\]</span></p>
<p>Now, differentiate (y) with respect to (x):</p>
<p><span class="math display">\[
\frac{dy}{dx} = \frac{d}{dx} \left( (1 + e^{-x})^{-1} \right)
\]</span></p>
<p>Using the chain rule, where ( u = 1 + e^{-x} ) and ( y = u^{-1} ), we
get:</p>
<p><span class="math display">\[
\frac{dy}{dx} = -1 \cdot u^{-2} \cdot \frac{du}{dx}
\]</span></p>
<p>Now, differentiate ( u ) with respect to (x):</p>
<p><span class="math display">\[
\frac{du}{dx} = \frac{d}{dx} (1 + e^{-x}) = 0 + (-e^{-x}) = -e^{-x}
\]</span></p>
<p>Substitute () back into the derivative:</p>
<p><span class="math display">\[
\frac{dy}{dx} = - \frac{1}{(1 + e^{-x})^2} \cdot (-e^{-x})
\]</span></p>
<p>This simplifies to:</p>
<p><span class="math display">\[
\frac{dy}{dx} = \frac{e^{-x}}{(1 + e^{-x})^2}
\]</span></p>
<p>Notice that ( (x) = ), so:</p>
<p><span class="math display">\[
\frac{dy}{dx} = \frac{e^{-x}}{1 + e^{-x}} \cdot \frac{1}{1 + e^{-x}} =
\sigma(x) \cdot \left(1 - \sigma(x)\right)
\]</span></p>
<p>Therefore, the derivative of the sigmoid function is:</p>
<p><span class="math display">\[
\frac{d\sigma(x)}{dx} = \sigma(x) \cdot (1 - \sigma(x))
\]</span></p>
</blockquote>
<p>在计算的时候可以使用for
loop实现，但是计算起来效率很低，因此可以使用向量化来计算。</p>
<h3 id="vectorization">Vectorization</h3>
<p>如果想计算for i in range t: z += w[i] * x[i] +
b[i]的话可以使用np.dot函数来代替。</p>
<h2 id="shallow-neural-networks">Shallow Neural Networks</h2>
<p>神经网络中中括号的上标[1] [2]代表神经网络的层数，第几层。</p>
<h3 id="neural-network-representation">Neural Network
Representation</h3>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024082810140555%E2%80%AFPM.png"
alt="image-2024082810140555 PM" />
<figcaption aria-hidden="true">image-2024082810140555 PM</figcaption>
</figure>
<p>最左边一列是输入层input layer</p>
<p>第二列是隐藏层hidden layer，这层里的数据是看不到的</p>
<p>第三列是输出层output layer</p>
<p>我们可以用<span
class="math inline">\(a^{[0]}=x\)</span>代表第一层，那么<span
class="math inline">\(\hat {y} = a^{[2]}\)</span></p>
<p>The number of rows in W[k] is the number of neurons in the k-th layer
and the number of columns is the number of inputs of the layer.</p>
<p>The number of columns in Z[1] and <em>A</em>[1] is equal to the
number of examples in the batch, m. And the number of rows inZ*[1] and
A[1] is equal to the number of neurons in the first layer.</p>
<h3 id="针对单一案例的神经网络训练">针对单一案例的神经网络训练</h3>
<p>假设现在有一个神经网络，第一层有四个node，每一个节点中都进行一次逻辑回归，在多层网络中计算方式为：
<span class="math display">\[
z_1^{[1]} = w_1^{[1]T} x + b_1^{[1]}, \quad a_1^{[1]} =
\sigma(z_1^{[1]})
\]</span></p>
<p><span class="math display">\[
z_2^{[1]} = w_2^{[1]T} x + b_2^{[1]}, \quad a_2^{[1]} =
\sigma(z_2^{[1]})
\]</span></p>
<p>这里第一层的第一个node用自己的w和b，然后结果再用sigmoid处理得到结果a。</p>
<p>在向量化计算的时候，首先W矩阵是一个4,3的矩阵。每一行都是三个针对每一个特征x的不一样的w值，代表这个node的w值，有四行代表第一层的四个node。这里x有1,2,3是因为这个特征需要用三个值来表示，例如点的坐标xyz值。这里x数量也可以有很多，如在图像中就可能对于每一个输入都有64个x，就需要在每个node里有64个w，这时X就是（64,1），W就是（4,64）如果第一层有4个结点。</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024082934711496%E2%80%AFPM.png"
alt="image-2024082934711496 PM" />
<figcaption aria-hidden="true">image-2024082934711496 PM</figcaption>
</figure>
<p>这里乘法全都是矩阵点乘。</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024082935258057%E2%80%AFPM.png"
alt="image-2024082935258057 PM" />
<figcaption aria-hidden="true">image-2024082935258057 PM</figcaption>
</figure>
<p>这里x也可以写成<span
class="math inline">\(A^0\)</span>,下一步就是用A1矩阵计算。</p>
<h3 id="针对多个输入的神经网络">针对多个输入的神经网络</h3>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024082940213100%E2%80%AFPM.png"
alt="image-2024082940213100 PM" />
<figcaption aria-hidden="true">image-2024082940213100 PM</figcaption>
</figure>
<p>正向传播的时候流程如下： for (i = 1) to (m):</p>
<p><span class="math display">\[
z^{[1]}(i) = W^{[1]} x(i) + b^{[1]}
\]</span></p>
<p><span class="math display">\[
a^{[1]}(i) = \sigma(z^{[1]}(i))
\]</span></p>
<p><span class="math display">\[
z^{[2]}(i) = W^{[2]} a^{[1]}(i) + b^{[2]}
\]</span></p>
<p><span class="math display">\[
a^{[2]}(i) = \sigma(z^{[2]}(i))
\]</span></p>
<p>对于每个x都计算一下。</p>
<p>在矢量化实现的时候，首先需要有一个X矩阵，他是所有x按列堆叠产物。结果也是Z和a按列堆叠得到的。</p>
<h3 id="activation-functions">Activation functions</h3>
<p>Sigmoid：<span class="math inline">\(\sigma(z) =
1/(1+e^{-z})\)</span></p>
<p>Tanh: <span class="math inline">\(a = \frac{e^z -
e^{-z}}{e^z+e^{-z}}\)</span></p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024082955238814%E2%80%AFPM.png"
alt="image-2024082955238814 PM" />
<figcaption aria-hidden="true">image-2024082955238814 PM</figcaption>
</figure>
<p>ReLU: a = max(0,z)</p>
<p>在隐藏层使用tanhx或relu，在输出层使用sigmoid，因为我们想要输出范围是0-1.</p>
<h3 id="神经网络的反向传播">神经网络的反向传播</h3>
<p>反向传播的时候首先需要计算激活函数的导数：</p>
<p>sigmoid:</p>
<p><span class="math inline">\(g(z) = 1/(1+e^{-z})\)</span></p>
<p>g'(z) = g(z)(1-g(z))</p>
<p>Tanh:</p>
<p><span class="math inline">\(g(z) = \frac{e^z -
e^{-z}}{e^z+e^{-z}}\)</span></p>
<p><span class="math inline">\(g&#39;(z) = 1 - g(z)^2\)</span></p>
<p>ReLU: g(z) = max(0,z)</p>
<p>g'(z) = 0/1</p>
<p>然后需要使用Gradient descent</p>
<p>假设只有一层隐藏层，有如下参数：<span
class="math inline">\(w^{[1]}\)</span><span
class="math inline">\(b^{[1]}\)</span><span
class="math inline">\(w^{[2]}\)</span><span
class="math inline">\(b^{[2]}\)</span></p>
<p>输入有<span class="math inline">\(n_x\)</span>个特征，<span
class="math inline">\(n_1\)</span>个隐藏单元和<span
class="math inline">\(n_2\)</span>个输出单元。</p>
<p>我们有如下cost function：<span
class="math inline">\(J((w^{[1]},b^{[1]},w^{[2]},b^{[2]})) = 1/m
\sum^m_{i=1}L(\hat y,y)\)</span></p>
<p>这里<span class="math inline">\(\hat
y\)</span>是输出的预测，也就是<span
class="math inline">\(a^[2]\)</span>.</p>
<h3 id="计算梯度下降">计算梯度下降</h3>
<p>首先随机获取参数w和b</p>
<p>重复：</p>
<ol type="1">
<li>计算从i到m的对于参数的预测值<span class="math inline">\(\hat
y\)</span></li>
<li>计算dw1，db1，dw2，db2</li>
<li>更新参数，w1 = w1 - <span class="math inline">\(\alpha
dw^1\)</span>, b1 = b1 - <span class="math inline">\(\alpha
db^1\)</span></li>
</ol>
<p>直到到达预设的迭代次数。</p>
<h3 id="代码部署">代码部署</h3>
<p>案例：给一系列点坐标，做二分分类</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/download.png"
alt="download" />
<figcaption aria-hidden="true">download</figcaption>
</figure>
<p>实现一个如下图所示的模型：两个输入是因为每个点有x，y两个值</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/classification_kiank.png"
alt="classification_kiank" />
<figcaption aria-hidden="true">classification_kiank</figcaption>
</figure>
<p><strong>Reminder</strong>: The general methodology to build a Neural
Network is to: 1. Define the neural network structure ( # of input
units, # of hidden units, etc). 2. Initialize the model's parameters 3.
Loop: - Implement forward propagation - Compute loss - Implement
backward propagation to get the gradients - Update parameters (gradient
descent)</p>
<ol type="1">
<li><h4 id="初始化参数">初始化参数</h4>
<p>先定义模型的每一层大小，输入层通过输入的数据数量判断，输出层通过标签数据判断，隐藏层硬编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: layer_sizes</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span>(<span class="params">X, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    n_x -- the size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- the size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- the size of the output layer</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#(≈ 3 lines of code)</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>]</span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure>
<p>然后初始化w和b数组，这里需要注意对于w需要使用randn函数，因为这样可以生成遵循标准正态分布的随机数，即都在0周围。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span>(<span class="params">n_x, n_h, n_y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>    </span><br><span class="line">    <span class="comment">#(≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></li>
</ol>
<p>有了w和b我们部署循环，即</p>
<ol type="1">
<li>进行正向传播</li>
<li>计算损失函数</li>
<li>反向传播获取梯度</li>
<li>使用梯度进行梯度下降更新w和b</li>
</ol>
<h4 id="正向传播">正向传播：</h4>
<p>使用如下等式：均为矩阵点乘</p>
<p><span class="math display">\[Z^{[1]} =  W^{[1]} X +
b^{[1]}\tag{1}\]</span> <span class="math display">\[A^{[1]} =
\tanh(Z^{[1]})\tag{2}\]</span> <span class="math display">\[Z^{[2]} =
W^{[2]} A^{[1]} + b^{[2]}\tag{3}\]</span> <span
class="math display">\[\hat{Y} = A^{[2]} =
\sigma(Z^{[2]})\tag{4}\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION:forward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span>(<span class="params">X, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters (output of initialization function)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    <span class="comment">#(≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    <span class="comment"># (≈ 4 lines of code)</span></span><br><span class="line">    Z1 = np.dot(W1,X)+b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2,A1)+b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = &#123;<span class="string">&quot;Z1&quot;</span>: Z1,</span><br><span class="line">             <span class="string">&quot;A1&quot;</span>: A1,</span><br><span class="line">             <span class="string">&quot;Z2&quot;</span>: Z2,</span><br><span class="line">             <span class="string">&quot;A2&quot;</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure>
<h4 id="计算损失函数">计算损失函数：</h4>
<p>对于预测值数组A2使用如下等式：</p>
<p><span class="math display">\[J = - \frac{1}{m} \sum\limits_{i =
1}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) +
(1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)}
\small\tag{13}\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span>(<span class="params">A2, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost given in equation (13)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- &quot;true&quot; labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost given equation (13)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of examples</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    <span class="comment"># (≈ 2 lines of code)</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(<span class="number">1</span> - A2), (<span class="number">1</span> - Y))</span><br><span class="line">    cost = -np.<span class="built_in">sum</span>(logprobs)/m</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    cost = <span class="built_in">float</span>(np.squeeze(cost))  <span class="comment"># makes sure cost is the dimension we expect. </span></span><br><span class="line">                                    <span class="comment"># E.g., turns [[17]] into 17 </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h4 id="部署反向传播grad_summary">部署反向传播：<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/grad_summary.png"
alt="grad_summary" /></h4>
<p>在向量化实现中使用右侧的六个等式来计算。g1在这里是tanh激活函数，导数是1-g<sup>2,所以这里就是1-A1</sup>2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span>(<span class="params">parameters, cache, X, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the backward propagation using the instructions above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing our parameters </span></span><br><span class="line"><span class="string">    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;.</span></span><br><span class="line"><span class="string">    X -- input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- &quot;true&quot; labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.</span></span><br><span class="line">    <span class="comment">#(≈ 2 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary &quot;cache&quot;.</span></span><br><span class="line">    <span class="comment">#(≈ 2 lines of code)</span></span><br><span class="line">    A1 = cache[<span class="string">&quot;A1&quot;</span>]</span><br><span class="line">    A2 = cache[<span class="string">&quot;A2&quot;</span>]</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    <span class="comment">#(≈ 6 lines of code, corresponding to 6 equations on slide above)</span></span><br><span class="line">    dZ2 = A2-Y</span><br><span class="line">    dW2 = np.dot(dZ2,A1.T)/m</span><br><span class="line">    db2 = np.<span class="built_in">sum</span>(dZ2,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)/m</span><br><span class="line">    dZ1 = np.dot(W2.T,dZ2)*(<span class="number">1</span>-np.power(A1,<span class="number">2</span>))</span><br><span class="line">    dW1 = np.dot(dZ1,X.T)/m</span><br><span class="line">    db1 = np.<span class="built_in">sum</span>(dZ1,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)/m</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">&quot;dW1&quot;</span>: dW1,</span><br><span class="line">             <span class="string">&quot;db1&quot;</span>: db1,</span><br><span class="line">             <span class="string">&quot;dW2&quot;</span>: dW2,</span><br><span class="line">             <span class="string">&quot;db2&quot;</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<h4 id="更新参数">更新参数</h4>
<p><strong>General gradient descent rule</strong>: <span
class="math inline">\(\theta = \theta - \alpha \frac{\partial J }{
\partial \theta }\)</span> where <span
class="math inline">\(\alpha\)</span> is the learning rate and <span
class="math inline">\(\theta\)</span> represents a parameter.</p>
<p>注意记得乘学习率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span>(<span class="params">parameters, grads, learning_rate = <span class="number">1.2</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Updates parameters using the gradient descent update rule given above</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Retrieve a copy of each parameter from the dictionary &quot;parameters&quot;. Use copy.deepcopy(...) for W1 and W2</span></span><br><span class="line">    <span class="comment">#(≈ 4 lines of code)</span></span><br><span class="line">    oW1 = copy.deepcopy(parameters[<span class="string">&quot;W1&quot;</span>])</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    W2 = copy.deepcopy(parameters[<span class="string">&quot;W2&quot;</span>])</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary &quot;grads&quot;</span></span><br><span class="line">    <span class="comment">#(≈ 4 lines of code)</span></span><br><span class="line">    dW1 = grads[<span class="string">&quot;dW1&quot;</span>]</span><br><span class="line">    db1 = grads[<span class="string">&quot;db1&quot;</span>]</span><br><span class="line">    dW2 = grads[<span class="string">&quot;dW2&quot;</span>]</span><br><span class="line">    db2 = grads[<span class="string">&quot;db2&quot;</span>]</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="comment">#(≈ 4 lines of code)</span></span><br><span class="line">    W1 = oW1 - dW1*learning_rate</span><br><span class="line">    b1 = b1 - db1*learning_rate</span><br><span class="line">    W2 = W2 - dW2*learning_rate</span><br><span class="line">    b2 = b2 - db2*learning_rate</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<h4 id="组合代码">组合代码</h4>
<p>使用上述代码实现model()函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: nn_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span>(<span class="params">X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    num_iterations -- Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    <span class="comment">#(≈ 1 line of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x,n_h,n_y)</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):</span><br><span class="line">         </span><br><span class="line">        <span class="comment">#(≈ 4 lines of code)</span></span><br><span class="line">        <span class="comment"># Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: &quot;A2, Y&quot;. Outputs: &quot;cost&quot;.</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate = <span class="number">1.2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;Cost after iteration %i: %f&quot;</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<h4 id="使用模型进行预测">使用模型进行预测</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">parameters, X</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span></span><br><span class="line">    <span class="comment">#(≈ 2 lines of code)</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># YOUR CODE STARTS HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># YOUR CODE ENDS HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure>
<h2 id="deep-neural-network">Deep Neural Network</h2>
<p>层数=hidden+ output，用L来表示。用<span
class="math inline">\(n^{[L]}\)</span>表示第L层单元的数量。</p>
<p>N1 = 第一个隐藏层</p>
<p>使用for循环计算每一层的正向传播。</p>
<h3 id="神经网络中矩阵形状">神经网络中矩阵形状</h3>
<p>对每一层参数矩阵，有</p>
<p><span class="math inline">\(W^{[L]}:(n^{[L]},n^{[L-1]})\)</span></p>
<p><span class="math inline">\(b^{[L]}:(n^{[L]},1)\)</span></p>
<p><span class="math inline">\(dW^{[L]}:(n^{[L]},n^{[L-1]})\)</span></p>
<p><span class="math inline">\(db^{[L]}:(n^{[L]},1)\)</span></p>
<p>输入有m个的时候，X矩阵是<span
class="math inline">\((n^{[0]},m)\)</span>大小的</p>
<p>因此:</p>
<p><span class="math inline">\(Z^{[1]}:(n^{[1]},m)\)</span></p>
<p><span class="math inline">\(dZ^{[1]}:(n^{[1]},m)\)</span></p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024090115219715%E2%80%AFAM.png"
alt="image-2024090115219715 AM" />
<figcaption aria-hidden="true">image-2024090115219715 AM</figcaption>
</figure>
<h3 id="为什么深度网络效率高">为什么深度网络效率高？</h3>
<p>在深度网络中，每一层可以从细微到宏观地识别各种特征。</p>
<p>如第一层可以用来检测边缘，下一层检测更多特征。</p>
<h3
id="深度神经网络中的正向和反向传播">深度神经网络中的正向和反向传播</h3>
<h4 id="理论基础">理论基础</h4>
<p>General forward propagation:</p>
<p>Input <span class="math inline">\(a^{[l-1]}\)</span>,output <span
class="math inline">\(a^{[l]}\)</span></p>
<p>By the equation: <span class="math display">\[
Z^{[l]} = \omega^{[l]}a^{[l-1]}+b^{[l]}
\]</span></p>
<p><span class="math display">\[
a^{[l]} = g^{[l]}(Z^{[l]})
\]</span></p>
<p>这时可以cache一个Z(l)在一会的反向传播使用。</p>
<p>General backward propagation:</p>
<p>Input <span class="math inline">\(da^{[l]}\)</span>,output <span
class="math inline">\(da^{[l-1]}\)</span></p>
<p>这时会用到刚刚的Z(l),然后使用输出的da来更新dw和db。</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024090121119895%E2%80%AFAM.png"
alt="image-2024090121119895 AM" />
<figcaption aria-hidden="true">image-2024090121119895 AM</figcaption>
</figure>
<h4 id="具体实现">具体实现：</h4>
<p>正向传播使用这个公式计算</p>
<p><span class="math display">\[
Z^{[l]} = \omega^{[l]}A^{[l-1]}+b^{[l]}
\]</span></p>
<p><span class="math display">\[
A^{[l]} = g^{[l]}(Z^{[l]})
\]</span></p>
<p>反向传播： <span class="math display">\[
dZ^{[l]} = dA^{[l]}*g^{&#39;[l]}(Z^{[l]})
\]</span></p>
<p><span class="math display">\[
d\omega^{[l]} = \frac{1}{m}dZ^{[l]}A^{[l-1]T}
\]</span></p>
<p><span class="math display">\[
db^{[l]} = \frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True)
\]</span></p>
<p><span class="math display">\[
dA^{[l-1]} = \omega^{[l]T}dZ^{[l]}
\]</span></p>
<h3 id="parameters-v.-hyperparameters">Parameters v.
Hyperparameters</h3>
<p>Parameters: W,b</p>
<p>Hyperparameters: learning rate, iterations, hiddenlayers....</p>
<h1 id="improving-deep-neural-networks">Improving Deep Neural
Networks</h1>
<h2 id="practical-aspects-of-deep-learning">Practical Aspects of Deep
Learning</h2>
<h3 id="setting-up-machine-learning-application">Setting up Machine
Learning Application</h3>
<p>对手里的数据进行划分，一部分训练一部分测试。</p>
<p>如果数据集很小的话可以用60/20/20的比例分割训练，开发，测试集</p>
<p>如果数据集很大的话只需要一点来做测试和开发就可以。</p>
<h3 id="biasvariance">Bias/Variance</h3>
<p>Bias高会导致欠拟合，Var高会导致过拟合，具体表现为：如果训练集误差很低，测试集误差很高，那就是过拟合。如果都高且差的不多就是欠拟合。</p>
<p>Bayes Error:
在分类问题中，针对某个样本的最小误差。这个误差与数据本身的不确定性有关，他代表理论上无法超越的分类性能下限，无论用什么分类器都不能把误差降到贝叶斯误差以下。</p>
<p>bias大可以通过更大的模型/更久的训练时间来解决</p>
<p>var大可以通过更多数据/正则化regularization解决</p>
<h3 id="regularization正则化">Regularization正则化</h3>
<p>用于解决过拟合问题</p>
<p>在逻辑回归模型中，正则化有以下公式： <span class="math display">\[
J(w,b)=\frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)},
y^{(i)})+\frac{\lambda}{2m} \|\omega^{[l]}\|_2^2
\]</span> 后边这一项叫做<span
class="math inline">\(L_2\)</span>范数，他是模型中所有权重的平方和。</p>
<p>在深度神经网络中，通过添加lambda参数来实现：</p>
<ol type="1">
<li>损失函数 (J)：</li>
</ol>
<p><span class="math display">\[
J(\omega^{[1]}, b^{[1]}, \ldots, \omega^{[L]}, b^{[L]}) = \frac{1}{m}
\sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}
\sum_{l=1}^{L} \|\omega^{[l]}\|_2^2
\]</span></p>
<ol start="2" type="1">
<li><p>Frobenius 范数 ：</p>
<p>他是网络中所有w的平方和。</p></li>
</ol>
<p><span class="math display">\[
\|\omega^{[l]}\|_F^2 = \sum_{i=1}^{n^{[l+1]}} \sum_{j=1}^{n^{[l]}}
(\omega_{ij}^{[l]})^2
\]</span></p>
<ol start="3" type="1">
<li>权重矩阵的维度 ：</li>
</ol>
<p><span class="math display">\[
\omega^{[l]} : (n^{[l+1]}, n^{[l]})
\]</span></p>
<h3 id="dropout-redularization">Dropout Redularization</h3>
<p>在训练过程中随机“丢弃”一部分神经元来提高模型的泛化能力。</p>
<p>具体实现时，以固定概率p来随机丢弃神经元，被丢弃的神经元在这一次迭代中不会参与正向和反向传播。每次迭代中丢弃的神经元是随机选择的，这样模型就不会依赖某些特定的神经元。常用的p是0.2-0.5之间，丢弃只会在训练过程中使用。</p>
<h3 id="data-augmentation">Data augmentation</h3>
<p>对旧数据进行修改来训练可以获得更多训练集来解决过拟合问题。</p>
<p>图片中可以进行如翻转或裁剪。</p>
<h3 id="early-stopping">Early stopping</h3>
<p>在训练过程中监控模型在验证集上的性能，当性能不在改善的时候就停止训练来防止模型过拟合。</p>
<h3 id="归一化输入数据">归一化输入数据</h3>
<p>用来加快训练速度 <span class="math display">\[
x_{norm}=\frac{x - \mu}{\sigma}
\]</span></p>
<p>假如输入X是一些坐标(x1,x2),可以对所有输入减去他们的平均值<span
class="math inline">\(\mu =
\frac{1}{m}\sum_{i=1}^mx^\)</span>可以让数据都到中心位置。</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024090315808843%E2%80%AFAM.png"
alt="image-2024090315808843 AM" />
<figcaption aria-hidden="true">image-2024090315808843 AM</figcaption>
</figure>
<p>这时发现x1的方差比x2大，我们还需要再归一化方差：</p>
<p><span class="math inline">\(\sigma^2 =
\frac{1}{N}\sum^N_{i=1}(x_i-\mu)^2\)</span></p>
<p>开平方后得到标准差。</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024090320330291%E2%80%AFAM.png"
alt="image-2024090320330291 AM" />
<figcaption aria-hidden="true">image-2024090320330291 AM</figcaption>
</figure>
<p>如果数据比例不一样，如有的是0-1，有的是0-1000就可以进行归一化，不过做了归一化也没什么坏处。</p>
<h3 id="梯度消失梯度爆炸">梯度消失/梯度爆炸</h3>
<h3 id="gradient-checking">Gradient checking</h3>
<h2 id="optimization-algorithms">Optimization Algorithms</h2>
<h3 id="mini-batch-gradient-descent">Mini-batch gradient descent</h3>
<p>把输入的X，Y矩阵分成很多个小矩阵。</p>
<p>假设X是(n,m)大小，n是input
size，即样本的特征数量。m是样本的数量(number of
examples).Y是(a,m)大小，a是output
size，即输出类型，m是样本数量，即对每一个样本都有一个输出。</p>
<p>mini-batch的时候把m个样本分成很多份，然后用for循环对每个batch进行训练。每个batch的一轮处理叫做一个epoc。</p>
<p>mini-batch训练时的cost下降会有一些波动，因为具体效果取决于每个batch里数据，有可能有数据有噪音导致学习效果不好。</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024090422728986%E2%80%AFPM.png"
alt="image-2024090422728986 PM" />
<figcaption aria-hidden="true">image-2024090422728986 PM</figcaption>
</figure>
<p>在使用mini-batch的时候需要指定它的参数：每个batch里的数据大小。大小可以指定为64-1024之间的某个值</p>
<h3 id="exponentially-weighted-average指数加权平均数">Exponentially
weighted average指数加权平均数</h3>
<p>用来处理时间序列数据，他赋予最近的观测值最大的权重，最早的观测值最小的权重。
<span class="math display">\[
v_t = \beta v_{t-1} + (1 - \beta) x_t
\]</span></p>
<ul>
<li><span class="math inline">\(\beta\)</span>是平滑因子，范围0-1</li>
<li>vt是在t时间的数据的加权平均数</li>
<li>xt是时间t处的观测值</li>
</ul>
<p>可以用来计算一段时间的平均数据，如一个月内每天的平均气温。</p>
<p>但是用这个平均数直接计算会在一开始的时候有误差，这时就要使用误差修正（bias
correction）：</p>
<p>使用<span
class="math inline">\(\frac{v_t}{1-\beta^t}\)</span>来代替原来的vt可以修正这个问题。t很小的时候可以有更准确的数据，t变大的时候这个值和原来的vt几乎一样。</p>
<h3 id="gradient-descent-with-momentum">Gradient Descent with
momentum</h3>
<ol type="1">
<li>在每次迭代的时候，在每个mini-batch里计算dw和db</li>
<li>计算<span class="math inline">\(vd_w = \beta V_{dw} +
(1-\beta)d_w\)</span></li>
<li><span class="math inline">\(V_{db} = \beta V_{db}+
(1-\beta)d_b\)</span></li>
<li><span class="math inline">\(W = W-\alpha V_{dw}, b = b - \alpha
v_{db}\)</span></li>
</ol>
<p>一般设置<span class="math inline">\(\beta\)</span>为0.9</p>
<h3 id="rmsprop">RMSprop</h3>
<p>与momentum差不多，只不过<span class="math inline">\(S_{db}=\beta
S_{db} + (1-\beta)d_b^2\)</span></p>
<p>然后 <span class="math inline">\(w = w - \alpha
\frac{dw}{\sqrt{S_{dw}+\epsilon}}\)</span></p>
<h3 id="adam-optimization-algorithm">Adam Optimization Algorithm</h3>
<p>组合RMS和momentum</p>
<p>首先，初始化Vdb,Vdw,Sdb,Sdb都为0</p>
<p>然后在每次迭代中</p>
<ol type="1">
<li>计算<span class="math inline">\(vd_w = \beta V_{dw} +
(1-\beta)d_w\)</span>， <span class="math inline">\(V_{db} = \beta
V_{db}+ (1-\beta)d_b\)</span></li>
<li>计算<span class="math inline">\(S_{db}=\beta S_{db} +
(1-\beta)d_b^2\)</span>，</li>
</ol>
<p>注意这里两个beta不是一个。</p>
<ol start="3" type="1">
<li>应用bias correction , <span class="math inline">\(V_dw =
\frac{V_dw}{1-\beta^t}\)</span>,对四个参数都应用correction</li>
<li>更新w和b：<span class="math inline">\(w = w - \alpha
\frac{V_dw}{\sqrt{S_{dw}+\epsilon}}\)</span></li>
</ol>
<p>一般beta1(Vdw的参数)设置为0.9，beta2<span
class="math inline">\((Sdw^2)\)</span>设置为0.999，<span
class="math inline">\(\epsilon\)</span>设置为10^-8,这些也是Adam的默认参数。使用这些默认参数后调试学习率alpha来训练。</p>
<h3 id="learning-rate-decay">Learning Rate Decay</h3>
<p>在训练过程中可以逐渐减小学习率，因为后期不需要步长很长。</p>
<p>可以使用以下几种方法：</p>
<p><span class="math inline">\(\alpha = 0.95^{epochnum}
*\alpha_0\)</span></p>
<p><span class="math inline">\(\alpha =
\frac{k}{\sqrt{epochnum}}*\alpha_0\)</span></p>
<p>或者手动调整学习率。</p>
<h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2>
<h3 id="batch-normalization-批量归一化">Batch Normalization
批量归一化</h3>
<p>在之前讨论过归一化normalization问题，先减均值再除平均差。</p>
<p>现在需要对中间变量进行归一化。对Z(i)进行归一化处理。</p>
<p>例如：输入X后经过第一层得到Z[1],然后对Z1进行归一化后经过激活函数得到a1，然后用这个a1再进入下一层，以此类推。</p>
<h3 id="softmax-multi-class-classification-regression">Softmax
multi-class classification Regression</h3>
<p>输出可以有多个值，最后一层使用softmax层实现，也就是对上一层结果Z运用softmax激活函数。</p>
<p>具体来说，首先计算一个<span
class="math inline">\(t=e^{z[L]}\)</span>, 然后<span
class="math inline">\(a =
\frac{t}{\sum^4_{j=1}t_i}\)</span>.,其实就是算出每个结果对应的t之后求出他在所有求和之中站的比例，然后这个比例就是他是这个的概率。</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-20240906123946524%E2%80%AFPM.png"
alt="image-20240906123946524 PM" />
<figcaption aria-hidden="true">image-20240906123946524 PM</figcaption>
</figure>
<h3 id="intro-to-tensorflow">Intro to Tensorflow</h3>
<p>假设目前需要最小化cost function <span class="math inline">\(J =
(x-5)^2\)</span></p>
<p>用<code>w = tf.Variable(0,dtype=tf.float32)</code>来定义tf中的变量，即神经网络里的w。</p>
<figure>
<img
src="https://raw.githubusercontent.com/1982606762/picgo/master/image-2024090713618877%E2%80%AFAM.png"
alt="image-2024090713618877 AM" />
<figcaption aria-hidden="true">image-2024090713618877 AM</figcaption>
</figure>
<p>核心是定义cost function，</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Xuanlang
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.zhaoxuanlang.cn/2024/07/20/Deep-Learning/" title="Deep-Learning">https://www.zhaoxuanlang.cn/2024/07/20/Deep-Learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/07/07/Azure-learn/" rel="prev" title="Azure-learn">
                  <i class="fa fa-chevron-left"></i> Azure-learn
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/09/02/Web3-intro/" rel="next" title="Web3-intro">
                  Web3-intro <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">津ICP备19008018号-1 </a>
  </div>

<div class="copyright">
  &copy; 2019 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-award"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xuanlang</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/lozad@1.16.0/dist/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","single_dollars":true,"cjk_width":0.9,"normal_width":0.6,"append_css":true,"every_page":true,"js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/quicklink@2.2.0/dist/quicklink.umd.js" integrity="sha256-4kQf9z5ntdQrzsBC3YSHnEz02Z9C1UeW/E9OgnvlzSY=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://www.zhaoxuanlang.cn/2024/07/20/Deep-Learning/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"squirrel","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
